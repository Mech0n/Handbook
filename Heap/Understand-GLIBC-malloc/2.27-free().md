# 2.27 free()å‡½æ•°

>  2.27ä¸‹çš„free()å‡½æ•°ã€‚

é¦–å…ˆè¿›å…¥`_libc_free()`

### `__libc_free()`

1. é¦–å…ˆä¼šæ£€æŸ¥`__free_hook()`æ˜¯å¦ä¸ºç©ºã€‚
2. ç„¶åæ£€æŸ¥`free(0);`
3. ç„¶åä¼šæ£€æŸ¥`mmap`ä½.

```c
void
__libc_free (void *mem)
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    }

  if (mem == 0)                              /* free(0) has no effect */
    return;
```

ç¬¬ä¸€éƒ¨åˆ†æ˜¾ç„¶æ˜¯å°è¯•è°ƒç”¨`__free_hook()`å‡½æ•°ã€‚

ç„¶åæ£€æŸ¥æ˜¯å¦æ˜¯`free(0);`ï¼Œå¦‚æœæ˜¯å°±ç»“æŸå‡½æ•°ã€‚

```c
p = mem2chunk (mem);

if (chunk_is_mmapped (p))                       /* release mmapped memory. */
  {
    /* See if the dynamic brk/mmap threshold needs adjusting.
 Dumped fake mmapped chunks do not affect the threshold.  */
    if (!mp_.no_dyn_threshold
        && chunksize_nomask (p) > mp_.mmap_threshold
        && chunksize_nomask (p) <= DEFAULT_MMAP_THRESHOLD_MAX
  && !DUMPED_MAIN_ARENA_CHUNK (p))
      {
        mp_.mmap_threshold = chunksize (p);
        mp_.trim_threshold = 2 * mp_.mmap_threshold;
        LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                    mp_.mmap_threshold, mp_.trim_threshold);
      }
    munmap_chunk (p);
    return;
  }
```

ç„¶åè½¬æ¢`free()`çš„åœ°å€ä¸º`chunk`çš„å®é™…åœ°å€ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯`mmap()`å‡ºæ¥çš„ï¼Œå¦‚æœæ˜¯ï¼Œç›´æ¥é‡Šæ”¾ã€‚

ç„¶åä¼šåˆå§‹åŒ–`tcache bin`ã€‚

```c
# define MAYBE_INIT_TCACHE() \
  if (__glibc_unlikely (tcache == NULL)) \
    tcache_init();

[line 2921] static __thread tcache_perthread_struct *tcache = NULL; // å…¨å±€å˜é‡

MAYBE_INIT_TCACHE ();
```

è¿™éƒ¨åˆ†å¯ä»¥å‚è€ƒæˆ‘ä¹‹å‰å†™çš„[2.27-malloc()](./2.27-malloc().md#tcache_init)

>  **è¿™ä¸ªå‡½æ•°å°±æ˜¯ç”³è¯·ä¸€ä¸ª`0x240 + 0x10`çš„`chunk`æ¥ç®¡ç†`Tcache`ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨åˆ°çš„ç¬¬ä¸€ä¸ª`chunk`ï¼Œåº”è¯¥æ˜¯`tcache bin`è€Œç®¡ç†å…¶ä»–`bin`çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡`main_state`ã€‚**

æœ€ååˆ°äº†æˆ‘ä»¬çœŸæ­£å…³æ³¨çš„éƒ¨åˆ†ï¼š

```c
ar_ptr = arena_for_chunk (p);
_int_free (ar_ptr, p, 0);
```

### `__int_free()`

é¦–å…ˆçœ‹å£°æ˜å˜é‡ï¼Œå¹¶å¤ä¹ `malloc_state`ï¼Œ**å¤šäº†ä¸ª`have_fastchunks`å˜é‡ã€‚**

```c
struct malloc_state
{
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks; [ğŸ†•]

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

ç„¶åæ˜¯å‡½æ•°å¼€å¤´çš„å˜é‡ï¼š

```c
INTERNAL_SIZE_T size;        /* its size */
mfastbinptr *fb;             /* associated fastbin */
mchunkptr nextchunk;         /* next contiguous chunk */
INTERNAL_SIZE_T nextsize;    /* its size */
int nextinuse;               /* true if nextchunk is used */
INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
mchunkptr bck;               /* misc temp for linking */
mchunkptr fwd;               /* misc temp for linking */
```

æ²¡ä»€ä¹ˆå˜åŒ–ã€‚

#### é¿å…ä¸äº†çš„æ£€æŸ¥

1. æ£€æŸ¥`p`çš„ä½ç½®æ˜¯å¦å†…å­˜å¯¹é½ï¼Œä¸”å¤§å°åˆé€‚ï¼Œå¦å¤–å°±æ˜¯`size`çš„å¤§å°å¿…é¡»å¯¹é½ä¸”å¤§äºæœ€å°å—å¤§å°ã€‚
2. æ£€æŸ¥`p`æ˜¯å¦æ˜¯æ­£åœ¨ä½¿ç”¨çš„`chunk`ã€‚è¿™é‡Œä¼šæ£€æŸ¥`p`å‰ä¸€ä¸ª`chunk`çš„å¤§å°ï¼Œæ˜¯å¦ä¸`p`è¿æ¥ï¼Œå’Œåä¸€ä¸ª`chunk`çš„`pre_inuse`ä½å’Œåä¸€ä¸ªä¸ª`chunk`çš„å¤§å°æ˜¯å¦å¤§äºæœ€å°çš„`chunk`å¤§å°ã€‚

æ¥ä¸‹æ¥å¼€å§‹ç¬¬ä¸€ä¸ªæ£€æŸ¥ï¼š

æ£€æŸ¥`p`çš„ä½ç½®æ˜¯å¦å†…å­˜å¯¹é½ï¼Œä¸”å¤§å°åˆé€‚ï¼Œå¦å¤–å°±æ˜¯`size`çš„å¤§å°å¿…é¡»å¯¹é½ä¸”å¤§äºæœ€å°å—å¤§å°ã€‚

```c
size = chunksize (p);

/* Little security check which won't hurt performance: the
   allocator never wrapps around at the end of the address space.
   Therefore we can exclude some size values which might appear
   here by accident or by "design" from some intruder.  */
if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0)
    || __builtin_expect (misaligned_chunk (p), 0))
  malloc_printerr ("free(): invalid pointer");
/* We know that each chunk is at least MINSIZE bytes in size or a
   multiple of MALLOC_ALIGNMENT.  */
if (__glibc_unlikely (size < MINSIZE || !aligned_OK (size)))
  malloc_printerr ("free(): invalid size");
```

ç„¶åæ˜¯æ£€æŸ¥`p`æ˜¯å¦æ˜¯æ­£åœ¨ä½¿ç”¨çš„`chunk`ï¼Œè¿™é‡Œä¼šæ£€æŸ¥`p`å‰ä¸€ä¸ª`chunk`çš„å¤§å°ï¼Œæ˜¯å¦ä¸`p`è¿æ¥ï¼Œå’Œåä¸€ä¸ª`chunk`çš„`pre_inuse`ä½å’Œåä¸€ä¸ªä¸ª`chunk`çš„å¤§å°æ˜¯å¦å¤§äºæœ€å°çš„`chunk`å¤§å°ã€‚

```c
check_inuse_chunk(av, p);

# define check_inuse_chunk(A, P)        do_check_inuse_chunk (A, P)

#define inuse(p)							      \
  ((((mchunkptr) (((char *) (p)) + ((p)->size & ~SIZE_BITS)))->size) & PREV_INUSE)

#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

static void
do_check_inuse_chunk (mstate av, mchunkptr p)
{
  mchunkptr next;

  do_check_chunk (av, p);

  if (chunk_is_mmapped (p))
    return; /* mmapped chunks have no next/prev */

  /* Check whether it claims to be in use ... */
  assert (inuse (p));

  next = next_chunk (p);

  /* ... and is surrounded by OK chunks.
     Since more things can be checked with free chunks than inuse ones,
     if an inuse chunk borders them and debug is on, it's worth doing them.
   */
  if (!prev_inuse (p))
    {
      /* Note that we cannot even look at prev unless it is not inuse */
      mchunkptr prv = prev_chunk (p);
      assert (next_chunk (prv) == p);
      do_check_free_chunk (av, prv);
    }

  if (next == av->top)
    {
      assert (prev_inuse (next));
      assert (chunksize (next) >= MINSIZE);
    }
  else if (!inuse (next))
    do_check_free_chunk (av, next);
}
```

#### `tcache`

```c
#if USE_TCACHE
  {
    size_t tc_idx = csize2tidx (size);

    if (tcache
	&& tc_idx < mp_.tcache_bins
	&& tcache->counts[tc_idx] < mp_.tcache_count)
      {
	tcache_put (p, tc_idx);
	return;
      }
  }
#endif
```

å¦‚æœ`p`çš„å¤§å°åœ¨`tcache bin`èŒƒå›´å†…ï¼Œå¹¶ä¸”`tcache bin`æ²¡æœ‰æ»¡ï¼Œå°±ä¼šç›´æ¥è¢«æ‰”è¿›`tcache bin`ã€‚

#### `Fastbin && åˆå¹¶`

1. æ£€æŸ¥ä¸‹ä¸€ä¸ª`chunk`çš„å¤§å°æ˜¯å¦åœ¨åˆé€‚èŒƒå›´å†…`(2 * SIZE_SZ , av->system_mem)`
2. å°†`p`æ”¾åˆ°`fastbin`é‡Œï¼Œè¿™é‡Œä¼šæ£€æŸ¥Double Freeï¼Œæ£€æŸ¥fastbiné‡Œçš„ç¬¬ä¸€ä¸ª`chunk`æ˜¯å¦æ˜¯`p`ã€‚
3. æ£€æŸ¥`p`æ˜¯å¦æ˜¯`top_chunk`ï¼Œå¦‚æœæ˜¯ï¼Œå°±ä¼š`crash`ã€‚
4. ä¸”`nextchunk`çš„ä½ç½®ä¸èƒ½è¶…å‡º`top`çš„åœ°å€èŒƒå›´ã€‚æ£€æŸ¥`nextchunk`çš„`pre_inuse`æ˜¯å¦ä¸º`1`ã€‚æ£€æŸ¥`nextchunk`çš„å¤§å°æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ã€‚
5. 

é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨`Fastbin`èŒƒå›´å†…ã€‚

```c
if ((unsigned long)(size) <= (unsigned long)(get_max_fast ())
```

è¿æ¥ç¬¬ä¸€ä¸ªæ£€æŸ¥ï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ª`chunk`çš„å¤§å°æ˜¯å¦åœ¨åˆé€‚èŒƒå›´å†…`(2 * SIZE_SZ , av->system_mem)`

```c
  if (__builtin_expect (chunksize_nomask (chunk_at_offset (p, size))
      <= 2 * SIZE_SZ, 0)
|| __builtin_expect (chunksize (chunk_at_offset (p, size))
         >= av->system_mem, 0))
    {
bool fail = true;
/* We might not have a lock at this point and concurrent modifications
   of system_mem might result in a false positive.  Redo the test after
   getting the lock.  */
if (!have_lock)
  {
    __libc_lock_lock (av->mutex);
    fail = (chunksize_nomask (chunk_at_offset (p, size)) <= 2 * SIZE_SZ
      || chunksize (chunk_at_offset (p, size)) >= av->system_mem);
    __libc_lock_unlock (av->mutex);
  }

if (fail)
  malloc_printerr ("free(): invalid next size (fast)");
    }

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```

ç„¶åè¿™é‡Œå¥½åƒå½±å“ä¸å¤§ï¼Œä¸å…³æ³¨ã€‚

```c
free_perturb (chunk2mem(p), size - 2 * SIZE_SZ); 

static void
free_perturb (char *p, size_t n)
{
  if (__glibc_unlikely (perturb_byte))
    memset (p, perturb_byte, n);
}
```

æ‰¾åˆ°ç›¸åº”çš„fastbinçš„ç´¢å¼•`idx`ã€‚

```c
unsigned int idx = fastbin_index(size);
fb = &fastbin (av, idx);
```

ç„¶åå°†`p`æ”¾åˆ°`fastbin`é‡Œï¼Œè¿™é‡Œä¼šæ£€æŸ¥Double Freeï¼Œæ£€æŸ¥fastbiné‡Œçš„ç¬¬ä¸€ä¸ª`chunk`æ˜¯å¦æ˜¯`p`ï¼Œ

```c
  /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
  mchunkptr old = *fb, old2;

  if (SINGLE_THREAD_P)
    {
/* Check that the top of the bin is not the record we are going to
   add (i.e., double free).  */
if (__builtin_expect (old == p, 0))
  malloc_printerr ("double free or corruption (fasttop)");
p->fd = old;
*fb = p;
    }
  else
    do
{
  /* Check that the top of the bin is not the record we are going to
     add (i.e., double free).  */
  if (__builtin_expect (old == p, 0))
    malloc_printerr ("double free or corruption (fasttop)");
  p->fd = old2 = old;
}
    while ((old = catomic_compare_and_exchange_val_rel (fb, p, old2))
     != old2);

  /* Check that size of fastbin chunk at the top is the same as
     size of the chunk that we are adding.  We can dereference OLD
     only if we have the lock, otherwise it might have already been
     allocated again.  */
  if (have_lock && old != NULL
&& __builtin_expect (fastbin_index (chunksize (old)) != idx, 0))
    malloc_printerr ("invalid fastbin entry (free)");
}
```

**æ¥ä¸‹æ¥ä¼šå°è¯•åˆå¹¶,è¿™éƒ¨åˆ†æ˜¯ä½œå‡†å¤‡ï¼Œ**

ä¼šæ£€æŸ¥`p`æ˜¯å¦æ˜¯`top_chunk`ï¼Œå¦‚æœæ˜¯ï¼Œå°±ä¼š`crash`ï¼Œ

å¹¶ä¸”`nextchunk`çš„ä½ç½®ä¸èƒ½è¶…å‡º`top`çš„åœ°å€èŒƒå›´ã€‚æ£€æŸ¥`nextchunk`çš„`pre_inuse`æ˜¯å¦ä¸º`1`ã€‚æ£€æŸ¥`nextchunk`çš„å¤§å°æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ã€‚

```c
else if (!chunk_is_mmapped(p)) {

/* If we're single-threaded, don't lock the arena.  */
if (SINGLE_THREAD_P)
  have_lock = true;

if (!have_lock)
  __libc_lock_lock (av->mutex);

nextchunk = chunk_at_offset(p, size);

/* Lightweight tests: check whether the block is already the
   top block.  */
if (__glibc_unlikely (p == av->top))
  malloc_printerr ("double free or corruption (top)");
/* Or whether the next chunk is beyond the boundaries of the arena.  */
if (__builtin_expect (contiguous (av)
    && (char *) nextchunk
    >= ((char *) av->top + chunksize(av->top)), 0))
malloc_printerr ("double free or corruption (out)");
/* Or whether the block is actually not marked used.  */
if (__glibc_unlikely (!prev_inuse(nextchunk)))
  malloc_printerr ("double free or corruption (!prev)");

nextsize = chunksize(nextchunk);
if (__builtin_expect (chunksize_nomask (nextchunk) <= 2 * SIZE_SZ, 0)
|| __builtin_expect (nextsize >= av->system_mem, 0))
  malloc_printerr ("free(): invalid next size (normal)");

free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);
```

æ¥ä¸‹æ¥å¼€å§‹å°è¯•åˆå¹¶ï¼š

```c
/* consolidate backward */
if (!prev_inuse(p)) {
  prevsize = prev_size (p);
  size += prevsize;
  p = chunk_at_offset(p, -((long) prevsize));
  unlink(av, p, bck, fwd);
}
```

å¦‚æœ`pre_chunk`æ²¡æœ‰åœ¨ä½¿ç”¨ï¼Œå°±ä¼šä¸å…¶åˆå¹¶ã€‚

```c
/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)					      \
  (((mchunkptr) (((char *) (p)) + (s)))->mchunk_size & PREV_INUSE)

if (nextchunk != av->top) {
    /* get and clear inuse bit */
    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

    /* consolidate forward */
    if (!nextinuse) {
unlink(av, nextchunk, bck, fwd);
size += nextsize;
    } else
clear_inuse_bit_at_offset(nextchunk, 0);
```

å¦‚æœ`nextchunk`ä¸æ˜¯`top`ï¼Œæ£€æŸ¥`nextchunk`çš„`nextchunk`çš„`inuse`ä½ï¼Œå¦‚æœè¯å®`nextchunk`ä¸åœ¨ä½¿ç”¨ï¼Œå°±ä¼šå°è¯•åˆå¹¶ã€‚

```c
    /*
Place the chunk in unsorted chunk list. Chunks are
not placed into regular bins until after they have
been given one chance to be used in malloc.
    */

    bck = unsorted_chunks(av);
    fwd = bck->fd;
    if (__glibc_unlikely (fwd->bk != bck))
malloc_printerr ("free(): corrupted unsorted chunks");
    p->fd = fwd;
    p->bk = bck;
    if (!in_smallbin_range(size))
{
  p->fd_nextsize = NULL;
  p->bk_nextsize = NULL;
}
    bck->fd = p;
    fwd->bk = p;

    set_head(p, size | PREV_INUSE);
    set_foot(p, size);

    check_free_chunk(av, p); //æœ‰ä¸€ä¸ªæ£€æŸ¥ï¼Œè¿™ä¸ªä¸éœ€è¦å’±ä»¬æ“ä½œã€‚
  }
```

ç„¶åæŠŠè¿™ä¸ªåˆå¹¶å®Œä¹‹åå°±ä¼šæŠŠè¿™ä¸ªæ–°`chunk`æ”¾åˆ°`Unsorted bin`ä¸­ï¼Œæœ‰ä¸€ä¸ª`Unsorted bin`å®Œæ•´æ€§çš„å¸¸è§„æ£€æŸ¥ã€‚

```c
else {
  size += nextsize;
  set_head(p, size | PREV_INUSE);
  av->top = p;
  check_chunk(av, p);
}
```

å¦‚æœæ˜¯`top`ï¼Œå°±ä¼šå’Œ`top`åˆå¹¶ã€‚

ğŸ¤”ï¸å¦‚æœ`size`è¿‡å¤§ï¼Œå°±ä¼šåˆå¹¶`Fastbin`ã€‚

```c
#define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)

  if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
    if (atomic_load_relaxed (&av->have_fastchunks))
malloc_consolidate(av);
```

#### æ¥ä¸‹æ¥ä¸€æ®µğŸ¦

```c

      if (av == &main_arena) {
#ifndef MORECORE_CANNOT_TRIM
	if ((unsigned long)(chunksize(av->top)) >=
	    (unsigned long)(mp_.trim_threshold))
	  systrim(mp_.top_pad, av);
#endif
      } else {
	/* Always try heap_trim(), even if the top chunk is not
	   large, because the corresponding heap might go away.  */
	heap_info *heap = heap_for_ptr(top(av));

	assert(heap->ar_ptr == av);
	heap_trim(heap, mp_.top_pad);
      }
    }

    if (!have_lock)
      __libc_lock_unlock (av->mutex);
  }
  /*
    If the chunk was allocated via mmap, release via munmap().
  */

  else {
    munmap_chunk (p);
  }
}
```

