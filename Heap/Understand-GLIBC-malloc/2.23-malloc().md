# GLIBC malloc.c æºç å­¦ä¹ 

> é¦–å…ˆæ˜¯[glibc2.23/mlloc.c](https://github.com/Mech0n/Handbook/blob/master/malloc/malloc-2.23/malloc.c)

**å‡½æ•°åˆ—è¡¨ï¼š**

1. `malloc(size_t n);`(å½“å‰ page)
2. `free(void* p);        `
3. `calloc(size_t n_elements, size_t element_size);`
4. `realloc(void* p, size_t n);        `
5. `memalign(size_t alignment, size_t n);   `     
6. `valloc(size_t n);        `
7. `mallinfo();     `
8. `mallopt(int parameter_number, int parameter_value)`

###  0x1 `mallocï¼ˆsize_t nï¼‰`

**æè¿°**ï¼šè¿”å›ä¸€ä¸ªæŒ‡å‘æ–°åˆ†é…çš„è‡³å°‘`n`ä¸ªå­—èŠ‚çš„å—çš„æŒ‡é’ˆï¼›

å¦‚æœæ²¡æœ‰å¯ç”¨ç©ºé—´ï¼Œåˆ™è¿”å›`null`ã€‚ 

æ­¤å¤–ï¼Œå‡ºç°æ•…éšœæ—¶ï¼Œ`errno`ä¸ºåœ¨ANSI Cç³»ç»Ÿä¸Šè®¾ç½®ä¸º`ENOMEM`ã€‚ 

å¦‚æœ`n`ä¸º`0`ï¼Œåˆ™`malloc`è¿”å›æœ€å°å¤§å°çš„å—ã€‚ ï¼ˆåœ¨å¤§å¤šæ•°32ä½ç³»ç»Ÿä¸Šï¼Œæœ€å°å¤§å°ä¸º16 bytesï¼Œåœ¨64ä½ç³»ç»Ÿä¸Šï¼Œæœ€å°å¤§å°ä¸º24æˆ–32 bytesã€‚ï¼‰

ç”±äºè¿™æ ·çš„è®¾å®šï¼š

`strong_alias (__libc_malloc, __malloc) strong_alias (__libc_malloc, malloc)`ï¼Œ

æ‰€ä»¥`__libc_malloc(size_t)`æ‰æ˜¯`malloc`å‡½æ•°ã€‚

`void*  __libc_malloc(size_t);`:

```c
void *
__libc_malloc (size_t bytes)
{
  mstate ar_ptr;
  void *victim;

  void *(*hook) (size_t, const void *)
    = atomic_forced_read (__malloc_hook);
  if (__builtin_expect (hook != NULL, 0))
    return (*hook)(bytes, RETURN_ADDRESS (0));

  arena_get (ar_ptr, bytes);

  victim = _int_malloc (ar_ptr, bytes);
  /* Retry with another arena only if we were able to find a usable arena
     before.  */
  if (!victim && ar_ptr != NULL)
    {
      LIBC_PROBE (memory_malloc_retry, 1, bytes);
      ar_ptr = arena_get_retry (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
    }

  if (ar_ptr != NULL)
    (void) mutex_unlock (&ar_ptr->mutex);

  assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
          ar_ptr == arena_for_chunk (mem2chunk (victim)));
  return victim;
}
libc_hidden_def (__libc_malloc)
```

é¦–å…ˆçœ‹`atomic_forced_read`ï¼Œï¼ˆè¿™éƒ¨åˆ†ç›®å‰è¿˜ä¸å…³æ³¨ï¼Œæš‚ä¸”æ‘˜æŠ„è‡ª[è¿™é‡Œ]([https://introspelliam.github.io/2018/05/21/pwn/malloc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94ptmalloc/](https://introspelliam.github.io/2018/05/21/pwn/mallocæºç åˆ†æâ€”ptmalloc/)))

```c
# define atomic_forced_read(x) \
  ({ __typeof (x) __x; __asm ("" : "=r" (__x) : "0" (x)); __x; })
```

`__typeof`æ˜¯åŸå§‹å‡½æ•°çš„è¿”å›ç±»å‹ï¼Œåé¢æ˜¯ä¸€æ®µæ±‡ç¼–ä»£ç ï¼Œâ€0â€æ˜¯é›¶ï¼Œå³%0ï¼Œå¼•ç”¨æ—¶ä¸å¯ä»¥åŠ  %ï¼Œåªèƒ½inputå¼•ç”¨outputï¼Œè¿™é‡Œå°±æ˜¯åŸå­è¯»ï¼Œå°†`__malloc_hook`çš„åœ°å€æ”¾å…¥ä»»æ„å¯„å­˜å™¨(r)å†å–å‡ºã€‚`__malloc_hook`çš„å®šä¹‰å¦‚ä¸‹

```c
void *weak_variable (*__malloc_hook)(size_t __size, const void *) = malloc_hook_ini;
```

weak_variableå…¶å®å°±æ˜¯ï¼Œ

```c
__attribute__ ((weak))
```

å’Œç¼–è¯‘å™¨æœ‰å…³ï¼Œè¿™é‡Œä¸ç®¡å®ƒã€‚`__builtin_expect`å…¶å®å°±æ˜¯å‘Šè¯‰ç¼–è¯‘å™¨ifåˆ¤æ–­è¯­å¥é‡Œå¤§å¤šæ•°æƒ…å†µä¸‹çš„å€¼ï¼Œè¿™æ ·ç¼–è¯‘å™¨å¯ä»¥åšä¼˜åŒ–ï¼Œé¿å…è¿‡å¤šçš„è·³è½¬ã€‚å›åˆ°`__libc_malloc`æ¥ä¸‹æ¥å°±æ˜¯è°ƒç”¨`malloc_hook_ini`è¿›è¡Œå†…å­˜çš„åˆ†é…ã€‚
`malloc_hook_ini`å®šä¹‰åœ¨hooks.cä¸­ï¼Œ

```c
static void * malloc_hook_ini (size_t sz, const void *caller){
    __malloc_hook = NULL;
    ptmalloc_init ();
    return __libc_malloc (sz);
}
```

#### `arena_get()`

æ¥ä¸‹æ¥æ˜¯`arena_get`:

å †å†…å­˜çš„è¿ç»­åŒºåŸŸè¢«ç§°ä¸ºã€Œarenaã€ã€‚è¿™ä¸ª arena æ˜¯ç”±ä¸»çº¿ç¨‹åˆ›å»ºï¼Œåˆ™è¢«ç§°ä¸º[main arena](https://github.com/sploitfun/lsploits/blob/master/glibc/malloc/malloc.c#L1740)ã€‚è¿›ä¸€æ­¥çš„åˆ†é…è¯·æ±‚ä¼šç»§ç»­ä½¿ç”¨è¿™ä¸ª arena ç›´åˆ° [arena ç©ºé—²ç©ºé—´è€—å°½](https://github.com/sploitfun/lsploits/blob/master/glibc/malloc/malloc.c#L3788)ã€‚å½“ arena è€—å°½ç©ºé—²ç©ºé—´ æ—¶ï¼Œå®ƒèƒ½åœ¨çº¿ç¨‹è°ƒç”¨é«˜çº§åˆ«çš„ä¸­æ–­çš„ä½ç½®æ—¶å»ºç«‹ï¼ˆè°ƒæ•´å»ºç«‹å¼€å§‹å—çš„ [size](https://github.com/sploitfun/lsploits/blob/master/glibc/malloc/malloc.c#L2521) ä»¥åŒ…å«é¢å¤–ç©ºé—´ä¹‹åï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œarena ä¹Ÿèƒ½åœ¨ top chunk æœ‰å¤§é‡ç©ºé—²ç©ºé—´æ—¶[å›æ”¶](https://github.com/sploitfun/lsploits/blob/master/glibc/malloc/malloc.c#L4044)ã€‚

[arena_get()](https://github.com/Mech0n/Handbook/blob/master/malloc/malloc-2.23/arena.c)

```c
/* arena_get() acquires an arena and locks the corresponding mutex.
   First, try the one last locked successfully by this thread.  (This
   is the common case and handled with a macro for speed.)  Then, loop
   once over the circularly linked list of arenas.  If no arena is
   readily available, create a new one.  In this latter case, `size'
   is just a hint as to how much memory will be required immediately
   in the new arena. */

#define arena_get(ptr, size) do { \
      ptr = thread_arena;						      \
      arena_lock (ptr, size);						      \
  } while (0)
```

#### `_int_malloc()`

ç„¶åè¿›å…¥`_int_malloc()`

åœ¨ç¬¬ä¸€æ¬¡`malloc`ä¹‹å‰ä¼šç”Ÿæˆä¸€ä¸ª`main_state`:

```c
struct malloc_state
{
  /* Serialize access.  */
  mutex_t mutex;

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

é¦–å…ˆæ˜¯ä¸€äº›å£°æ˜å˜é‡ï¼š

```c
INTERNAL_SIZE_T nb;               /* normalized request size æ ‡å‡†è¯·æ±‚å¤§å°(åé¢ä¼šè¢«set)*/
unsigned int idx;                 /* associated bin index ç›¸å…³binç´¢å¼•*/
/*
/* Forward declarations.  */
struct malloc_chunk;
typedef struct malloc_chunk* mchunkptr;
typedef struct malloc_chunk *mbinptr;
*/
mbinptr bin;                      /* associated bin ç›¸å…³bin(æŒ‡é’ˆ)*/

mchunkptr victim;                 /* inspected/selected chunk é€‰æ‹©çš„chunk*/
INTERNAL_SIZE_T size;             /* its size å…¶å¤§å°*/
int victim_index;                 /* its bin index æ‰€åœ¨binçš„ç´¢å¼•*/

mchunkptr remainder;              /* remainder from a split æ‹†åˆ†chunkä¹‹åçš„å‰©ä½™éƒ¨åˆ†*/
unsigned long remainder_size;     /* its size å…¶å¤§å°*/

unsigned int block;               /* bit map traverser */
unsigned int bit;                 /* bit map traverser */
unsigned int map;                 /* current word of binmap */

mchunkptr fwd;                    /* misc temp for linking */
mchunkptr bck;                    /* misc temp for linking */

const char *errstr = NULL;
```

ç„¶åè¿›å…¥å‡½æ•°ï¼šï¼ˆå…ˆè¡¥å……ä¸€äº›å®å®šä¹‰ï¼‰

```c
//MALLOC_ALIGNMENTæ˜¯mallocåˆ†é…çš„å—çš„æœ€å°å¯¹é½æ–¹å¼ã€‚å³ä½¿åœ¨æœºå™¨ä¸Šï¼Œå®ƒä¹Ÿå¿…é¡»æ˜¯è‡³å°‘2 * SIZE_SZçš„2çš„å¹‚ï¼Œ
//ä¸ºæ­¤ï¼Œè¾ƒå°çš„å¯¹é½æ–¹å¼å°±è¶³å¤Ÿäº†ã€‚
#  define MALLOC_ALIGNMENT       (2 *SIZE_SZ < __alignof__ (long double)      \
                                  ? __alignof__ (long double) : 2 *SIZE_SZ)
# else
#  define MALLOC_ALIGNMENT       (2 *SIZE_SZ)


/* The corresponding bit mask value */
#define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)

/* The smallest possible chunk */
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))

/* The smallest size we can malloc is an aligned minimal chunk */

#define MINSIZE  \
  (unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK))
/*
   Check if a request is so large that it would wrap around zero when
   padded and aligned. To simplify some other code, the bound is made
   low enough so that adding MINSIZE will also not wrap around zero.
 */
// æ£€æŸ¥è¯·æ±‚å¤§å°æ˜¯å¦è¿‡å¤§
#define REQUEST_OUT_OF_RANGE(req)                                 \
  ((unsigned long) (req) >=						      \
   (unsigned long) (INTERNAL_SIZE_T) (-2 * MINSIZE))

/* pad request bytes into a usable size -- internal version */

#define request2size(req)                                         \
  (((req) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
   MINSIZE :                                                      \
   ((req) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)

/*  Same, except also perform argument check */

#define checked_request2size(req, sz)                             \
  if (REQUEST_OUT_OF_RANGE (req)) {					      \
      __set_errno (ENOMEM);						      \
      return 0;								      \
    }									      \
  (sz) = request2size (req);
```

é¦–å…ˆï¼Œ

```c
// ------------------------------------------------------------
/*
   Convert request size to internal form by adding SIZE_SZ bytes
   overhead plus possibly more to obtain necessary alignment and/or
   to obtain a size of at least MINSIZE, the smallest allocatable
   size. Also, checked_request2size traps (returning 0) request sizes
   that are so large that they wrap around zero when padded and
   aligned.
 */
//è¯·æ±‚çš„å¤§å°è½¬æ¢æˆchunkå¤§å°ï¼Œéœ€è¦å…ˆæ£€æŸ¥å¤§å°æ˜¯å¦è¶Šç•Œã€‚ç„¶åå°† nb è½¬æ¢ä¸º chunk sizeã€‚
checked_request2size (bytes, nb);
```

ç„¶åä¼šæ£€æŸ¥`arena`ï¼Œå¦‚æœæ²¡æœ‰è¢«åˆå§‹åŒ–ï¼Œåˆ™ä¼šé‡æ–°ä»`mmap`å¾—åˆ°`chunk`ã€‚

```c
/* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
   mmap.  */
if (__glibc_unlikely (av == NULL))
  {
    void *p = sysmalloc (nb, av);
    if (p != NULL)
alloc_perturb (p, bytes);
    return p;
  }
```

ç„¶åä¼šå»æ£€æŸ¥`bytes`æ˜¯å¦ç¬¦åˆ`fastbin`çš„å¤§å°ï¼Œå¹¶å°è¯•åˆ†é…ã€‚

```c
/*
     If the size qualifies as a fastbin, first check corresponding bin.
     This code is safe to execute even if av is not yet initialized, so we
     can try it without checking, which saves some time on this fast path.
   */

  if ((unsigned long) (nb) <= (unsigned long) (get_max_fast ()))
    {
      idx = fastbin_index (nb);
      mfastbinptr *fb = &fastbin (av, idx);
      mchunkptr pp = *fb;
      do
        {
          victim = pp;
          if (victim == NULL)
            break;
        }
      while ((pp = catomic_compare_and_exchange_val_acq (fb, victim->fd, victim))
             != victim);
      if (victim != 0)
        {
          if (__builtin_expect (fastbin_index (chunksize (victim)) != idx, 0))
            {
              errstr = "malloc(): memory corruption (fast)";
            errout:
              malloc_printerr (check_action, errstr, chunk2mem (victim), av);
              return NULL;
            }
          check_remalloced_chunk (av, victim, nb);
          void *p = chunk2mem (victim);
          alloc_perturb (p, bytes);	//è°ƒè¯•ç”¨ï¼Œä¸å…³å¿ƒã€‚
          return p;
        }
    }
```

è¡¥å……å®å®šä¹‰ï¼š

```c
#define DEFAULT_MXFAST     (64 * SIZE_SZ / 4) == 

static void	//ä¼šè¢«æå‰ä½¿ç”¨æ¥åˆå§‹åŒ–ä¸€ä¸ªarena(av)
malloc_init_state (mstate av)
{
  int i;
  mbinptr bin;

  /* Establish circular links for normal bins */
  for (i = 1; i < NBINS; ++i)
    {
      bin = bin_at (av, i);
      bin->fd = bin->bk = bin;
    }

#if MORECORE_CONTIGUOUS
  if (av != &main_arena)
#endif
  set_noncontiguous (av);
  if (av == &main_arena)
    set_max_fast (DEFAULT_MXFAST);
  av->flags |= FASTCHUNKS_BIT;

  av->top = initial_top (av);
}

static INTERNAL_SIZE_T global_max_fast;
/* Maximum size of memory handled in fastbins.  */
// åœ¨malloc_init_stateè°ƒç”¨ï¼Œè§ä¸Šé¢
#define set_max_fast(s) \
  global_max_fast = (((s) == 0)                           \
                     ? SMALLBIN_WIDTH : ((s + SIZE_SZ) & ~MALLOC_ALIGN_MASK))
#define get_max_fast() global_max_fast
```

æ‰€ä»¥ï¼Œåœ¨x86_64ä¸Š`fastbin`çš„æœ€å¤§å€¼æ˜¯`0x80`,è€Œx86ä¸Šæ˜¯`0x40`ã€‚

å¦‚æœåˆ¤æ–­æ˜¯éœ€æ±‚`fastbin`èŒƒå›´å†…çš„`chunk`ï¼Œ

è·å¾—`fastbin`çš„`idx`ï¼Œå¹¶åˆ›å»ºæŒ‡é’ˆåªæƒ³è¿™ä¸ª`bin`

```c
/* offset 2 to use otherwise unindexable first 2 bins */
#define fastbin_index(sz) \
  ((((unsigned int) (sz)) >> (SIZE_SZ == 8 ? 4 : 3)) - 2)

#define fastbin(ar_ptr,idx) ((ar_ptr)->fastbinsY[idx])\
	((av)->fastbinsY[idx])
```

`pp`æŒ‡å‘`bin`ä¸­çš„ç¬¬ä¸€ä¸ª`chunk`ï¼Œç„¶å`do`ã€`while`å¾ªç¯æ˜¯ä¸€ä¸ªCASæ“ä½œï¼Œå…¶ä½œç”¨æ˜¯ä»åˆšåˆšå¾—åˆ°çš„ç©ºé—²`chunk`é“¾è¡¨æŒ‡é’ˆä¸­å–å‡ºç¬¬ä¸€ä¸ªç©ºé—²çš„`chunk(victim)`ï¼Œå¹¶å°†é“¾è¡¨å¤´è®¾ç½®ä¸ºè¯¥ç©ºé—²`chunk`çš„ä¸‹ä¸€ä¸ª`chunk(victim->fd)`ã€‚

##### æ£€æŸ¥ `Fastbin`

1. `victim`çš„`size`è¦æ»¡è¶³`idx`ã€‚
2. `chunk`çš„`size`çš„æ ‡å¿—ä½åˆæ³•ã€‚(`mmap`ä½)
3. **è¿™é‡Œå‚»äº†ï¼Œhouse of spirité‡Œçš„next chunkæ£€æŸ¥æ˜¯åœ¨`free()`é‡Œçš„æ£€æŸ¥ã€‚**

æ¥ä¸‹æ¥ï¼Œå¦‚æœ`victim`ä¸æ˜¯ç©ºçš„ï¼Œä¹Ÿå°±æ˜¯è¯´`fastbin`é‡Œæœ‰`chunk`ï¼Œé‚£ä¹ˆè¿›å…¥ä¸€ä¸ªæ£€æŸ¥,ï¼Œæ£€æŸ¥`victim`çš„`size`æ˜¯ä¸æ˜¯ç¬¦åˆ`idx`ã€‚

```c
__builtin_expect (fastbin_index (chunksize (victim)) != idx, 0)
```

ç„¶åæ˜¯ç¬¬äºŒä¸ªæ£€æŸ¥ï¼š

```c
check_remalloced_chunk (av, victim, nb);

# define check_remalloced_chunk(A, P, N) do_check_remalloced_chunk (A, P, N)

static void
do_check_remalloced_chunk (mstate av, mchunkptr p, INTERNAL_SIZE_T s)
{
  INTERNAL_SIZE_T sz = p->size & ~(PREV_INUSE | NON_MAIN_ARENA);

  if (!chunk_is_mmapped (p))	//æ£€æŸ¥chunkçš„mmapä½ï¼Œä¸é‡ç‚¹å…³æ³¨ã€‚
    {
      assert (av == arena_for_chunk (p));
      if (chunk_non_main_arena (p))
        assert (av != &main_arena);
      else
        assert (av == &main_arena);
    }

  do_check_inuse_chunk (av, p);

  /* Legal size ... */				//æ£€æŸ¥æ˜¯å¦æ˜¯åˆæ³•å¤§å°ã€‚sz >= MINSIZEï¼Œå¹¶ä¸”å†…å­˜å¯¹é½ã€‚
  assert ((sz & MALLOC_ALIGN_MASK) == 0);
  assert ((unsigned long) (sz) >= MINSIZE);
  /* ... and alignment */			//æ£€æŸ¥chunkåœ°å€æ˜¯å¦å¯¹é½
  assert (aligned_OK (chunk2mem (p)));
  /* chunk is less than MINSIZE more than request */	//æ£€æŸ¥æ˜¯å¦æ»¡è¶³è¯·æ±‚å¤§å°ï¼Œè€Œä¸”æ˜¯å¦ç»™çš„è¿‡å¤šã€‚
  assert ((long) (sz) - (long) (s) >= 0);
  assert ((long) (sz) - (long) (s + MINSIZE) < 0);
}
```

æ¥ä¸‹æ¥æ˜¯`small bin`æ®µã€‚ç›¸å…³å‡½æ•°[Glibcï¼šæµ…è°ˆ malloc_consolidate() å‡½æ•°å…·ä½“å®ç°](https://blog.csdn.net/plus_re/article/details/79265805)

```c
/*
     If a small request, check regular bin.  Since these "smallbins"
     hold one size each, no searching within bins is necessary.
     (For a large request, we need to wait until unsorted chunks are
     processed to find best fit. But for small ones, fits are exact
     anyway, so we can check now, which is faster.)
   */

  if (in_smallbin_range (nb))
    {
      idx = smallbin_index (nb);
      bin = bin_at (av, idx);

      if ((victim = last (bin)) != bin)	//å¦‚æœbin->bkæŒ‡å‘çš„ä¸æ˜¯binï¼Œè¯æ˜binéç©ºã€‚
        {
          if (victim == 0) /* initialization check */	//è¯æ˜arenaæ²¡æœ‰è¢«åˆå§‹åŒ–ã€‚
            malloc_consolidate (av);	//åˆå¹¶fastbinå¹¶æ”¾å…¥unsorted bin
          else
            {
              bck = victim->bk;
	if (__glibc_unlikely (bck->fd != victim))
                {
                  errstr = "malloc(): smallbin double linked list corrupted";
                  goto errout;
                }
              set_inuse_bit_at_offset (victim, nb);
              bin->bk = bck;	//ç±»ä¼¼unlinkæ“ä½œã€‚
              bck->fd = bin;

              if (av != &main_arena)	//æœ€åæ˜¯ä¸fastbinéƒ¨åˆ†ç›¸åŒçš„æ£€æŸ¥ã€‚
                victim->size |= NON_MAIN_ARENA;
              check_malloced_chunk (av, victim, nb);
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
            }
        }
    }
```

å…ˆæ£€æŸ¥èŒƒå›´æ˜¯å¦ç¬¦åˆ`small bin`èŒƒå›´ï¼šåŠå°äº`large bin`çš„ä¸‹ç•Œï¼ˆx86_64æ˜¯`0x400`ï¼‰ã€‚

```c
#define in_smallbin_range(sz)  \
  ((unsigned long) (sz) < (unsigned long) MIN_LARGE_SIZE)
```

æ¥ä¸‹æ¥æ˜¯æ‹¿åˆ°`bin`çš„`idx`ï¼Œç„¶åå¾—åˆ°ç›¸åº”`bin`çš„æŒ‡é’ˆã€‚

```c
#define SMALLBIN_CORRECTION (MALLOC_ALIGNMENT > 2 * SIZE_SZ)

#define smallbin_index(sz) \
  ((SMALLBIN_WIDTH == 16 ? (((unsigned) (sz)) >> 4) : (((unsigned) (sz)) >> 3))\
   + SMALLBIN_CORRECTION)
```

å–`bin`çš„æœ€åä¸€ä¸ª`chunk`(æœ€æ—©è¢«ä¸¢è¿›`bin`ä¸­çš„`chunk`)ã€‚å¦‚æœæœ‰`chunk`åˆ™èµ‹ç»™`victim`

```c
#define last(b)      ((b)->bk)
(victim = last (bin))
```

##### æ£€æŸ¥`small bin`

1. æ£€æŸ¥`victim->bk->fd`æ˜¯å¦æ˜¯`victim`ã€‚
2. æ£€æŸ¥`victim`æ˜¯å¦åˆæ³•(å’Œ`fastbinç›¸åŒ`)

ç„¶åæ£€æŸ¥`bck(victim->bk)->fd`æ˜¯å¦æ˜¯`victim`ã€‚

ç„¶åè®¾ç½®`chunk size`çš„`pre_inuse`æ ‡å¿—ä½ã€‚

```c
/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1
#define set_inuse_bit_at_offset(p, s)					      \
  (((mchunkptr) (((char *) (p)) + (s)))->size |= PREV_INUSE)
```

ç„¶åä¿®å¤`small bin`çš„å¾ªç¯é“¾è¡¨ã€‚

æœ€åæ˜¯å’Œ`fastbin`ä¸€æ ·çš„æ£€æŸ¥ã€‚

æ£€æŸ¥å®Œ`fastbin`å’Œ`small bin`ä¹‹åéƒ½æ²¡æœ‰`chunk`ç¬¦åˆè¦æ±‚ï¼Œå°±ä¼šåˆå¹¶`fastbin`ï¼Œå¹¶ä¸”å°†`bin`çš„ç´¢å¼•`idx`è®¾ç½®æˆ`large bin`çš„ç´¢å¼•ã€‚

```c
#define FASTCHUNKS_BIT        (1U)
#define have_fastchunks(M)     (((M)->flags & FASTCHUNKS_BIT) == 0)

else
  {
    idx = largebin_index (nb);
    if (have_fastchunks (av))
      malloc_consolidate (av);
  }
```

æ¥ä¸‹æ¥è¿›å…¥`Unsorted bin`é˜¶æ®µï¼Œ`fastbin`å·²ç»å…¨ç©ºã€‚

##### æ£€æŸ¥`Unsorted bin`

1. å¯¹`Unsorted bin`çš„æ¯ä¸€ä¸ª`chunk`ï¼Œä¼šæ£€æŸ¥å…¶`size`å­—æ®µæ˜¯å¦åˆæ³•ã€‚`[2 * SIZE_SZ, av->system_mem]`

   æ‰¾äº†ä¸€ä¸ªx64çš„ä¾‹å­ï¼š

   ```shell
   system_mem = 135168,
   max_system_mem = 135168
   
   âœ  ~ rax2 135168
   0x21000
   ```

   

```c
for (;; )
  {
    int iters = 0;
    //ä»Unsorted bin çš„æœ€å…ˆè¢«ä¸¢è¿›å»çš„é‚£ä¸ªchunkå¼€å§‹ã€‚
    while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
      {
        bck = victim->bk;
      //æ£€æŸ¥`victim`çš„`size`æ˜¯å¦å¤§å°åˆé€‚ï¼Œ[2 * SIZE_SZ, av->system_mem]
        if (__builtin_expect (victim->size <= 2 * SIZE_SZ, 0)
            || __builtin_expect (victim->size > av->system_mem, 0))
          malloc_printerr (check_action, "malloc(): memory corruption",
                           chunk2mem (victim), av);
        size = chunksize (victim);	//å¾—åˆ°victim->size
```

è¡¥å……å®ï¼š

```c
//Get size, ignoring use bits
#define chunksize(p) ((p)->size & ~(SIZE_BITS)) \
	((victim)->size & ~(SIZE_BITS))
```

å¦‚æœç”¨æˆ·éœ€è¦åˆ†é…çš„å†…å­˜å¤§å°å¯¹åº”çš„`chunk`å±äº`smallbin`ï¼Œ`unsortedbin`ä¸­åªæœ‰è¿™ä¸€ä¸ª`chunk`ï¼Œå¹¶ä¸”è¯¥`chunk`å±äº`last remainder chunk`ä¸”å…¶å¤§å°å¤§äºç”¨æˆ·éœ€è¦åˆ†é…å†…å­˜å¤§å°å¯¹åº”çš„`chunk`å¤§å°åŠ ä¸Šæœ€å°çš„`chunk`å¤§å°ï¼ˆä¿è¯å¯ä»¥æ‹†å¼€æˆä¸¤ä¸ª`chunk`ï¼‰ï¼Œå°±å°†è¯¥`chunk`æ‹†å¼€æˆä¸¤ä¸ª`chunk`ï¼Œåˆ†åˆ«ä¸º`victim`å’Œ`remainder`ï¼Œè¿›è¡Œç›¸åº”çš„è®¾ç½®åï¼Œå°†ç”¨æˆ·éœ€è¦çš„`victim`è¿”å›ã€‚
å¦‚æœä¸èƒ½æ‹†å¼€ï¼Œå°±ä»`unsorted bin`ä¸­å–å‡ºè¯¥`chunk`(`victim`)ã€‚

```c
/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)->size = (((p)->size & SIZE_BITS) | (s)))

/* Set size/use field */
#define set_head(p, s)       ((p)->size = (s))

/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))->prev_size = (s))

if (in_smallbin_range (nb) &&
    bck == unsorted_chunks (av) &&
    victim == av->last_remainder &&
    (unsigned long) (size) > (unsigned long) (nb + MINSIZE))
  {
    /* split and reattach remainder */
    remainder_size = size - nb;
    remainder = chunk_at_offset (victim, nb);
    unsorted_chunks (av)->bk = unsorted_chunks (av)->fd = remainder;
    av->last_remainder = remainder;
    remainder->bk = remainder->fd = unsorted_chunks (av);
    if (!in_smallbin_range (remainder_size))
      {
        remainder->fd_nextsize = NULL;
        remainder->bk_nextsize = NULL;
      }

    set_head (victim, nb | PREV_INUSE |
              (av != &main_arena ? NON_MAIN_ARENA : 0));
    set_head (remainder, remainder_size | PREV_INUSE);
    set_foot (remainder, remainder_size);		//è®¾ç½®ç©ºé—²chunkçš„pre_size

  	//å¸¸è§„æ£€æŸ¥ï¼Œç±»ä¼¼fastbin small bin
    check_malloced_chunk (av, victim, nb);
    void *p = chunk2mem (victim);
    alloc_perturb (p, bytes);
    return p;
  }
```

æ¥ä¸‹æ¥ï¼Œå°±è¯æ˜éå†`unsorted bin`ï¼ˆå®é™…ä¸Šåˆšæ‰çš„`remainder`ä¹Ÿåœ¨éå†è¿‡ç¨‹é‡Œé¢ï¼‰ï¼Œå–å‡ºæ¯ä¸€ä¸ª`chunk`ã€‚

```c
/* remove from unsorted list */
unsorted_chunks (av)->bk = bck;
bck->fd = unsorted_chunks (av);
```

å¦‚æœ`victim->size`æ­£å¥½æ˜¯æˆ‘ä»¬è¯·æ±‚çš„å¤§å°ï¼ˆæ ‡å‡†å¤§å°ï¼‰ï¼Œå°±è¿”å›è¿™ä¸ª`chunk`ã€‚

```c
if (size == nb)
  {
    set_inuse_bit_at_offset (victim, size);
    if (av != &main_arena)
      victim->size |= NON_MAIN_ARENA;
  //å¸¸è§„æ£€æŸ¥ï¼Œç±»ä¼¼fastbin ã€small bin
    check_malloced_chunk (av, victim, nb);
    void *p = chunk2mem (victim);
    alloc_perturb (p, bytes);
    return p;
  }
```

å¦‚æœåœ¨`small bin`çš„èŒƒå›´ï¼Œå°±æ”¾è¿›`small bin`ã€‚å¦åˆ™å°±æ”¾å…¥`large bin`

```c
if (in_smallbin_range (size))
  {
    victim_index = smallbin_index (size);
    bck = bin_at (av, victim_index);	//victim->bk
    fwd = bck->fd;										//victim->fd
  }
[Â·Â·Â·]
//æ”¾å…¥ç›¸åº”çš„bin
mark_bin (av, victim_index);
victim->bk = bck;
victim->fd = fwd;
fwd->bk = victim;
bck->fd = victim;
```

`large bin `éƒ¨åˆ†ï¼ˆè¯¦ç»†ä»‹ç»åœ¨å¦ä¸€ç¯‡æ–‡é‡Œï¼Œlargebin attackï¼‰

```c
else
  {
    victim_index = largebin_index (size);
    bck = bin_at (av, victim_index);	
    fwd = bck->fd;										

    /* maintain large bins in sorted order */
    if (fwd != bck)	//large bin ä¸ç©º
      {
        /* Or with inuse bit to speed comparisons */
        size |= PREV_INUSE;	//ä»ç„¶è¦æ ‡è®° pre_inuse
        /* if smaller than smallest, bypass loop below */
      	//æ£€æŸ¥NON_MAIN_ARENAä½
        assert ((bck->bk->size & NON_MAIN_ARENA) == 0);
      	//å¦‚æœvictimå¤§å°å°äºlarge binä¸­æœ€å°çš„ï¼Œå°±å•ç‹¬æˆæ’å…¥ã€‚
        if ((unsigned long) (size) < (unsigned long) (bck->bk->size))
          {
            fwd = bck;
            bck = bck->bk;

            victim->fd_nextsize = fwd->fd;
            victim->bk_nextsize = fwd->fd->bk_nextsize;
            fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
          }
        else
          {
            assert ((fwd->size & NON_MAIN_ARENA) == 0);
          //å¯»æ‰¾large binä¸­çš„åˆé€‚ä½ç½®ã€‚
            while ((unsigned long) size < fwd->size)
              {
                fwd = fwd->fd_nextsize;
                assert ((fwd->size & NON_MAIN_ARENA) == 0);
              }

            if ((unsigned long) size == (unsigned long) fwd->size)
              /* Always insert in the second position.  */
              fwd = fwd->fd;
            else
              {
                victim->fd_nextsize = fwd;
                victim->bk_nextsize = fwd->bk_nextsize;
                fwd->bk_nextsize = victim;
                victim->bk_nextsize->fd_nextsize = victim;
              }
            bck = fwd->bk;
          }
      }
    else	//å¦‚æœlarge bin ä¸ºç©ºã€‚
      victim->fd_nextsize = victim->bk_nextsize = victim;
  }
```

åœ¨`Unsorted bin `çš„ç»“å°¾åŸæ¥æœ‰é—å¿˜çš„åœ°æ–¹

```c
#define MAX_ITERS       10000
          if (++iters >= MAX_ITERS)
            break;
```

ç„¶åæœç´¢`large bin`

>  âš ï¸	**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç›´åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œå³ä½¿`Unsorted bin`ä¸­å­˜åœ¨æ¯”`size`æ›´å¤§çš„`chunk`è¶³ä»¥å»`split`å‡ºæ¥ä¸€ä¸ªæ»¡è¶³è¦æ±‚çš„`chunk`ï¼Œå¹¶ä¸”è¿™ä¸ªå—ä¸æ˜¯`last_remainder`, `Unsorted bin`å°±ä»ç„¶ä¸ä¼šå»splitè¿™ä¸ªå—ï¼Œè€Œæ˜¯æŠŠå®ƒæ”¾åˆ°å¯¹åº”çš„`small bin`æˆ–è€…`large bin`ä¸­ï¼Œç­‰å¾…æ‰«æå®Œ`large bin`ä¹‹åä»ç„¶æ²¡æœ‰è¿”å›ä¸€ä¸ªåˆé€‚çš„å—çš„æ—¶å€™ï¼Œå†å»åˆ‡å‰²ä¸€ä¸ªåˆé€‚çš„`chunk`ï¼Œå¹¶å»è®¾ç½®`last_remainder`**

##### æ£€æŸ¥`large bin`

1. å…¨ç¨‹åªæ˜¯`size`çš„æ¯”å¯¹ã€‚
2. æœ‰ä¸€ä¸ªæ£€æŸ¥ï¼Œåº”è¯¥ä¸æ˜¯å¾ˆé‡è¦ã€‚åœ¨å–å‡º`chunk`ä¹‹åï¼Œå‰©ä½™éƒ¨åˆ†æ”¾å…¥`Unsorted bin`æ—¶å€™ï¼Œä¸€å®šè¦ä¿è¯`Unsorted bin`å®Œæ•´ã€‚

```c
if (!in_smallbin_range (nb))
  {
  //ç´¢å¼•åˆ°bin
    bin = bin_at (av, idx);

    /* skip scan if empty or largest chunk is too small */
  	//é€‰æ‹©æœ€å¤§çš„chunkï¼Œæ£€æŸ¥æ˜¯å¦èŒƒå›´ç¬¦åˆï¼Œä¸”åˆ¤æ–­æ˜¯å¦å¯èƒ½å­˜åœ¨chunk
    if ((victim = first (bin)) != bin &&
        (unsigned long) (victim->size) >= (unsigned long) (nb))
      {
        victim = victim->bk_nextsize;
      
      //å¯»æ‰¾é‚£ä¸ªåˆé€‚å¤§å°çš„chunk
        while (((unsigned long) (size = chunksize (victim)) <
                (unsigned long) (nb)))
          victim = victim->bk_nextsize;

        /* Avoid removing the first entry for a size so that the skip
           list does not have to be rerouted.  */
      	//å¦‚æœæ‰¾åˆ°äº†åˆé€‚çš„chunkï¼Œé¿å…å–å‡ºé‚£ä¸ªç”¨æ¥ç´¢å¼•çš„ç¬¬ä¸€ä¸ªchunk
        if (victim != last (bin) && victim->size == victim->fd->size)
          victim = victim->fd;
				//æŠŠchunkçš„å‰©ä½™éƒ¨åˆ†æ”¾å…¥remainderã€‚å¹¶ä¸”unlink
        remainder_size = size - nb;
        unlink (av, victim, bck, fwd);
				
        /* Exhaust */
      	//remainder_sizeè¿‡å°ï¼Œç›´æ¥é€ç»™ç”¨æˆ·
        if (remainder_size < MINSIZE)
          {
            set_inuse_bit_at_offset (victim, size);
            if (av != &main_arena)
              victim->size |= NON_MAIN_ARENA;
          }
        /* Split */
      	//remainder_sizeè¿˜ä¸ç®—å°ï¼Œç•™åœ¨Unsorted biné‡Œ
        else
          {
            remainder = chunk_at_offset (victim, nb);
            /* We cannot assume the unsorted list is empty and therefore
               have to perform a complete insert here.  */
            bck = unsorted_chunks (av);
            fwd = bck->fd;
          //æœ‰ä¸ªæ£€æŸ¥ï¼ŒUnsorted binçš„åŒé“¾å¿…é¡»å®Œæ•´fwd->bk != bck
if (__glibc_unlikely (fwd->bk != bck))
              {
                errstr = "malloc(): corrupted unsorted chunks";
                goto errout;
              }
            remainder->bk = bck;
            remainder->fd = fwd;
            bck->fd = remainder;
            fwd->bk = remainder;
          //å¦‚æœè¿™ä¸ªè¢«åˆ‡å‰²è¿‡çš„chunkè¿˜åœ¨largebinçš„èŒƒå›´å†…ï¼Œå°±ä¼šæ¸…ç©ºfd_nextsizeå’Œbk_nextsize
            if (!in_smallbin_range (remainder_size))
              {
                remainder->fd_nextsize = NULL;
                remainder->bk_nextsize = NULL;
              }
            set_head (victim, nb | PREV_INUSE |
                      (av != &main_arena ? NON_MAIN_ARENA : 0));
            set_head (remainder, remainder_size | PREV_INUSE);
            set_foot (remainder, remainder_size);
          }
        check_malloced_chunk (av, victim, nb);
        void *p = chunk2mem (victim);
        alloc_perturb (p, bytes);
        return p;
      }
  }
```

ç„¶åæ£€æŸ¥ä¸‹ä¸€ä¸ª`large bin`

```c
#define BINMAPSHIFT      5
#define idx2block(i)     ((i) >> BINMAPSHIFT)
#define idx2bit(i)       ((1U << ((i) & ((1U << BINMAPSHIFT) - 1))))

++idx;
bin = bin_at (av, idx);
block = idx2block (idx);
map = av->binmap[block];
bit = idx2bit (idx);				//ç”¨æ¥æ ‡è®°è¿™ä¸ªlarge binæ˜¯å¦ä¸ºNULLï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è·³è¿‡æ‰«æã€‚
```

##### `largebin`ååŠéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†æš‚ä¸”ğŸ¦

è¿™ä¸€éƒ¨åˆ†çš„æ•´ä½“æ„æ€æ˜¯ï¼Œå‰é¢åœ¨`largebin`ä¸­å¯»æ‰¾ç‰¹å®šå¤§å°çš„ç©ºé—²`chunk`ï¼Œå¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿™é‡Œå°±è¦éå†largebinä¸­çš„å…¶ä»–æ›´å¤§çš„`chunk`åŒå‘é“¾è¡¨ï¼Œç»§ç»­å¯»æ‰¾ã€‚
å¼€å¤´çš„`++idx`å°±è¡¨ç¤ºï¼Œè¿™é‡Œè¦ä»`largebin`ä¸­ä¸‹ä¸€ä¸ªæ›´å¤§çš„`chunk`åŒå‘é“¾è¡¨å¼€å§‹éå†ã€‚ptmallocä¸­ç”¨ä¸€ä¸ª`bit`è¡¨ç¤º`malloc_state`çš„`bins`æ•°ç»„ä¸­å¯¹åº”çš„ä½ç½®ä¸Šæ˜¯å¦æœ‰ç©ºé—²`chunk`ï¼Œ`bit`ä¸º1è¡¨ç¤ºæœ‰ï¼Œä¸º0åˆ™æ²¡æœ‰ã€‚ptmallocé€šè¿‡4ä¸ª`block`ï¼ˆä¸€ä¸ªblock 4å­—èŠ‚ï¼‰ä¸€å…±128ä¸ªbitç®¡ç†`bins`æ•°ç»„ã€‚å› æ­¤ï¼Œä»£ç ä¸­è®¡ç®—çš„`block`è¡¨ç¤ºå¯¹åº”çš„`idx`å±äºå“ªä¸€ä¸ª`block`ï¼Œ`map`å°±è¡¨æ˜¯`block`å¯¹åº”çš„`bit`ç»„æˆçš„äºŒè¿›åˆ¶æ•°å­—ã€‚
æ¥ä¸‹æ¥è¿›å…¥`for`å¾ªç¯ï¼Œå¦‚æœ`bit > map`ï¼Œè¡¨ç¤ºè¯¥`map`å¯¹åº”çš„æ•´ä¸ª`block`é‡Œéƒ½æ²¡æœ‰å¤§äº`bit`ä½ç½®çš„ç©ºé—²çš„chunkï¼Œå› æ­¤å°±è¦æ‰¾ä¸‹ä¸€ä¸ª`block`ã€‚å› ä¸ºåé¢çš„`block`åªè¦ä¸ç­‰äº0ï¼Œå°±è‚¯å®šæœ‰ç©ºé—²`chunk`ï¼Œå¹¶ä¸”å…¶å¤§å°å¤§äº`bit`ä½ç½®å¯¹åº”çš„`chunk`ï¼Œä¸‹é¢å°±æ ¹æ®`block`ï¼Œå–å‡º`block`å¯¹åº”çš„ç¬¬ä¸€ä¸ªåŒå‘é“¾è¡¨çš„å¤´æŒ‡é’ˆã€‚è¿™é‡Œä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œè®¾ç½®`map`å’Œ`block`ä¹Ÿæ˜¯ä¸ºäº†åŠ å¿«æŸ¥æ‰¾çš„é€Ÿåº¦ã€‚å¦‚æœéå†å®Œæ‰€æœ‰`block`éƒ½æ²¡æœ‰ç©ºé—²chunkï¼Œè¿™æ—¶åªèƒ½ä»top chunké‡Œåˆ†é…chunkäº†ï¼Œå› æ­¤è·³è½¬åˆ°`use_top`ã€‚
å¦‚æœæœ‰ç©ºé—²chunkï¼Œæ¥ä¸‹æ¥å°±é€šè¿‡ä¸€ä¸ªwhileå¾ªç¯ä¾æ¬¡æ¯”è¾ƒæ‰¾å‡ºåˆ°åº•åœ¨å“ªä¸ªåŒå‘é“¾è¡¨é‡Œå­˜åœ¨ç©ºé—²chunkï¼Œæœ€åè·å¾—ç©ºé—²chunkæ‰€åœ¨çš„åŒå‘é“¾è¡¨çš„å¤´æŒ‡é’ˆ`bin`å’Œä½ç½®`bit`ã€‚
æ¥ä¸‹æ¥ï¼Œå¦‚æœæ‰¾åˆ°çš„åŒå‘é“¾è¡¨åˆä¸ºç©ºï¼Œåˆ™ç»§ç»­å‰é¢çš„éå†ï¼Œæ‰¾åˆ°ç©ºé—²chunkæ‰€åœ¨çš„åŒå‘é“¾è¡¨çš„å¤´æŒ‡é’ˆ`bin`å’Œä½ç½®`bit`ã€‚å¦‚æœæ‰¾åˆ°çš„åŒå‘é“¾è¡¨ä¸ä¸ºç©ºï¼Œå°±å’Œä¸Šé¢ä¸€éƒ¨åˆ†å†largebinä¸­æ‰¾åˆ°ç©ºé—²chunkçš„æ“ä½œä¸€æ ·äº†ï¼Œè¿™é‡Œå°±ä¸ç»§ç»­åˆ†æäº†ã€‚

æœ€åä¸€éƒ¨åˆ†æ˜¯`top chunk`ã€‚å½“ç°æœ‰çš„éƒ½æ²¡åŠæ³•æ»¡è¶³éœ€æ±‚ï¼Œå°±ä½¿ç”¨è¿™éƒ¨åˆ†ï¼Œ

```c
    use_top:
      /*
         If large enough, split off the chunk bordering the end of memory
         (held in av->top). Note that this is in accord with the best-fit
         search rule.  In effect, av->top is treated as larger (and thus
         less well fitting) than any other available chunk since it can
         be extended to be as large as necessary (up to system
         limitations).
         We require that av->top always exists (i.e., has size >=
         MINSIZE) after initialization, so if it would otherwise be
         exhausted by current request, it is replenished. (The main
         reason for ensuring it exists is that we may need MINSIZE space
         to put in fenceposts in sysmalloc.)
       */

      victim = av->top;
      size = chunksize (victim);

      if ((unsigned long) (size) >= (unsigned long) (nb + MINSIZE))
        {
        	//åˆ†é…ä¹‹åï¼Œå¤šä½™çš„éƒ¨åˆ†ä¹Ÿè¢«ä½œä¸ºremainder_size
        	//ä½†æ˜¯æ²¡æœ‰è¢«æ‰”è¿›Unsorted binï¼Œè€Œæ˜¯é‡æ–°æˆä¸ºav->top
          remainder_size = size - nb;
          remainder = chunk_at_offset (victim, nb);
          av->top = remainder;
          set_head (victim, nb | PREV_INUSE |
                    (av != &main_arena ? NON_MAIN_ARENA : 0));
          set_head (remainder, remainder_size | PREV_INUSE);

          check_malloced_chunk (av, victim, nb);
          void *p = chunk2mem (victim);
          alloc_perturb (p, bytes);
          return p;
        }

      /* When we are using atomic ops to free fast chunks we can get
         here for all block sizes.  */
			//æ£€æŸ¥fastbinä¸­æ˜¯å¦æœ‰ç©ºé—²å†…å­˜äº†ï¼ˆå…¶ä»–çº¿ç¨‹æ­¤æ—¶å¯èƒ½å°†é‡Šæ”¾çš„chunkæ”¾å…¥fastbinä¸­äº†ï¼‰.
			//å¦‚æœä¸ç©ºé—²ï¼Œåˆå¹¶fastbinä¸­çš„ç©ºé—²chunkå¹¶æ”¾å…¥smallbinæˆ–è€…largebinä¸­ï¼Œ
			//ç„¶åä¼šå›åˆ°_int_mallocå‡½æ•°ä¸­æœ€å‰é¢çš„forå¾ªç¯ã€‚
      else if (have_fastchunks (av))
        {
          malloc_consolidate (av);
          /* restore original bin index */
          if (in_smallbin_range (nb))
            idx = smallbin_index (nb);
          else
            idx = largebin_index (nb);
        }

      /*
         Otherwise, relay to handle system-dependent cases
       */
      else
        {
          void *p = sysmalloc (nb, av);
          if (p != NULL)
            alloc_perturb (p, bytes);
          return p;
        }
    }
}
```

