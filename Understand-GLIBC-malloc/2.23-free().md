# GLIBC malloc.c free()

> Glibc-2.23

**函数列表：**

1. `malloc(size_t n);`
2. `free(void* p);       `(当前 page) 
3. `calloc(size_t n_elements, size_t element_size);`
4. `realloc(void* p, size_t n);        `
5. `memalign(size_t alignment, size_t n);   `     
6. `valloc(size_t n);        `
7. `mallinfo();     `
8. `mallopt(int parameter_number, int parameter_value)`

#### `__libc_free()`

```c
void
__libc_free (void *mem)
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    }

  if (mem == 0)                              /* free(0) has no effect */
    return;

  p = mem2chunk (mem);

  if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
      /* see if the dynamic brk/mmap threshold needs adjusting */
      if (!mp_.no_dyn_threshold
          && p->size > mp_.mmap_threshold
          && p->size <= DEFAULT_MMAP_THRESHOLD_MAX)
        {
          mp_.mmap_threshold = chunksize (p);
          mp_.trim_threshold = 2 * mp_.mmap_threshold;
          LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                      mp_.mmap_threshold, mp_.trim_threshold);
        }
      munmap_chunk (p);
      return;
    }

  ar_ptr = arena_for_chunk (p);
  _int_free (ar_ptr, p, 0);
}
libc_hidden_def (__libc_free)
```

首先，会检查`__free_hook()`是否存在，如果存在，直接执行`__free_hook()`。

```c
#ifndef RETURN_ADDRESS
#define RETURN_ADDRESS(X_) (NULL)
#endif
```

接下来检测`free(NULL)`，不操作，直接退出函数。

然后` p = mem2chunk (mem);`获取`chunk`的带上`header`的地址。

然后的一段会检测`chunk`的`size`字段的`IS_MMAPPED`位，如果是`mmap`申请的内存，那么直接释放，（暂时不关注）

然后就到了我目前真正关注的地方了。

```c
//直接返回 main_arena
[line 1] ar_ptr = arena_for_chunk (p); 

#define arena_for_chunk(ptr) (chunk_non_main_arena (ptr) ? heap_for_ptr (ptr)->ar_ptr : &main_arena)

//调用_int_free()
[line 2] _int_free (ar_ptr, p, 0);
```

#### ` _int_free()`

先把`main_arena`再重复一下吧，万一用到呢。

```c
struct malloc_state
{
  /* Serialize access.  */
  mutex_t mutex;

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

然后看函数的开头的变量

```c
static void
_int_free (mstate av, mchunkptr p, int have_lock)
{
  INTERNAL_SIZE_T size;        /* its size */
  mfastbinptr *fb;             /* associated fastbin */ // fastbin
  mchunkptr nextchunk;         /* next contiguous chunk */ //next chunk(address)
  INTERNAL_SIZE_T nextsize;    /* its size */							 //nextchunk size 
  int nextinuse;               /* true if nextchunk is used */
  INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
  mchunkptr bck;               /* misc temp for linking */
  mchunkptr fwd;               /* misc temp for linking */

  const char *errstr = NULL;
  int locked = 0;

  size = chunksize (p);
  [···]
}

/* Get size, ignoring use bits */
#define chunksize(p)         ((p)->size & ~(SIZE_BITS))
```

##### 避免不了的检查 

1. `p`的地址对齐。
2. 要保证大小合适：`size > MINSIZE` 
3. `assert(inuse(p))`，从`next chunk`处检查`inuse(p)`是否已经被分配，如果检测到`chunk`没有被使用，那么就报错了。

首先会检查`p`的地址是否对齐。

```c
  if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0)
      || __builtin_expect (misaligned_chunk (p), 0))
    {
      errstr = "free(): invalid pointer";
    errout:
      if (!have_lock && locked)
        (void) mutex_unlock (&av->mutex);
      malloc_printerr (check_action, errstr, chunk2mem (p), av);
      return;
    }
```

然后检查`p->size`是否对齐，而且`> MINSIZE`

```c
if (__glibc_unlikely (size < MINSIZE || !aligned_OK (size)))
  {
    errstr = "free(): invalid size";
    goto errout;
  }
```

然后从`next chunk`处检查`inuse(p)`是否已经被分配，如果检测到`chunk`没有被使用，那么就报错了。

```c
# define check_inuse_chunk(A, P)        do_check_inuse_chunk (A, P)

#define inuse(p)							      \
  ((((mchunkptr) (((char *) (p)) + ((p)->size & ~SIZE_BITS)))->size) & PREV_INUSE)

static void
do_check_inuse_chunk (mstate av, mchunkptr p)
{
  mchunkptr next;

  do_check_chunk (av, p);

  if (chunk_is_mmapped (p))
    return; /* mmapped chunks have no next/prev */

  /* Check whether it claims to be in use ... */
  assert (inuse (p));

  next = next_chunk (p);

  /* ... and is surrounded by OK chunks.
     Since more things can be checked with free chunks than inuse ones,
     if an inuse chunk borders them and debug is on, it's worth doing them.
   */
  if (!prev_inuse (p))
    {
      /* Note that we cannot even look at prev unless it is not inuse */
      mchunkptr prv = prev_chunk (p);
      assert (next_chunk (prv) == p);
      do_check_free_chunk (av, prv);
    }

  if (next == av->top)
    {
      assert (prev_inuse (next));
      assert (chunksize (next) >= MINSIZE);
    }
  else if (!inuse (next))
    do_check_free_chunk (av, next);
}
```

##### Fastbin && 检查

1. 首先会检查`chunk`的大小合适，并且`next chunk`大小合适
2. 然后检查Double Free，检查`fastbin`里面的第一个`chunk`是否是`p`。

接下来是在Fastbin的操作，如果`chunk`在`Fastbin`范围内:

首先会检查`chunk`的大小合适，并且`next chunk`大小合适

```c
/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```

有一部分暂时不甚关系：

```c
	/* We might not have a lock at this point and concurrent modifications
	   of system_mem might have let to a false positive.  Redo the test
	   after getting the lock.  */
      //lock
	if (have_lock
	    || ({ assert (locked == 0);
		  mutex_lock(&av->mutex);
		  locked = 1;
		  chunk_at_offset (p, size)->size <= 2 * SIZE_SZ
		    || chunksize (chunk_at_offset (p, size)) >= av->system_mem;
	      }))
	  {
	    errstr = "free(): invalid next size (fast)";
	    goto errout;
	  }
	if (! have_lock)
	  {
	    (void)mutex_unlock(&av->mutex);
	    locked = 0;
	  }
      }

    //调试用。
    free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);
```

接下来将`main_arena`中`flag`标记为有`fastbin chunk`

```c
set_fastchunks(av);
```

然后取得相应的`fastbin`的索引`idx`和指针。

```c
#define fastbin(ar_ptr,idx) ((ar_ptr)->fastbinsY[idx])

unsigned int idx = fastbin_index(size);
fb = &fastbin (av, idx);
```

然后检查Double Free，检查`fastbin`里面的第一个`chunk`是否是`p`，然后通过CAS操作将`chunk`添加到`fastbin`中。

```c
  /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
  mchunkptr old = *fb, old2;
  unsigned int old_idx = ~0u;
  do
    {
/* Check that the top of the bin is not the record we are going to add
   (i.e., double free).  */
if (__builtin_expect (old == p, 0))
  {
    errstr = "double free or corruption (fasttop)";
    goto errout;
  }
/* Check that size of fastbin chunk at the top is the same as
   size of the chunk that we are adding.  We can dereference OLD
   only if we have the lock, otherwise it might have already been
   deallocated.  See use of OLD_IDX below for the actual check.  */
if (have_lock && old != NULL)
  old_idx = fastbin_index(chunksize(old));
p->fd = old2 = old;
    }
  while ((old = catomic_compare_and_exchange_val_rel (fb, p, old2)) != old2);

  if (have_lock && old != NULL && __builtin_expect (old_idx != idx, 0))
    {
errstr = "invalid fastbin entry (free)";
goto errout;
    }
}
```

在确定不是`fastbin`范围内，进入`else`代码段：

##### 非Fastbin && 检查

1. 检查`chunk`是否是`mmap`得到的
2. 检查`chunk`不是`av->top`
3. 然后检查`chunk`不能超过`arena`的边界，`nextchunk= av->top + chunksize(av->top)`
4. `nextchunk->size` :在`netx chunk->size`检查`p`是否没有被标记为已被分配
5. `nextchunk->size` :检查`next chunk`大小在合理范围内
6. 后向合并,并把新的`chunk`扔进`Unsorted bin`这里有个关于`Unsorted bin`完整性的检查。

检查`chunk`是否是`mmap`得到的：

```c
/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)

else if (!chunk_is_mmapped(p)) {
  if (! have_lock) {
    (void)mutex_lock(&av->mutex);
    locked = 1;
  }
```

然后获取`next chunk`的地址：

```c
nextchunk = chunk_at_offset(p, size);
```

检查`chunk`不是`av->top`:

```c
  /* Lightweight tests: check whether the block is already the
     top block.  */
  if (__glibc_unlikely (p == av->top))
    {
errstr = "double free or corruption (top)";
goto errout;
    }
```

然后检查`chunk`不能超过`arena`的边界

```c
  /* Or whether the next chunk is beyond the boundaries of the arena.  */
  if (__builtin_expect (contiguous (av)
      && (char *) nextchunk
      >= ((char *) av->top + chunksize(av->top)), 0))
    {
errstr = "double free or corruption (out)";
goto errout;
    }
```

在`netx chunk->size`检查`p`是否没有被标记为已被分配

```c
  if (__glibc_unlikely (!prev_inuse(nextchunk)))
    {
errstr = "double free or corruption (!prev)";
goto errout;
    }
```

检查`next chunk`大小在合理范围内：

```c
  nextsize = chunksize(nextchunk);
  if (__builtin_expect (nextchunk->size <= 2 * SIZE_SZ, 0)
|| __builtin_expect (nextsize >= av->system_mem, 0))
    {
errstr = "free(): invalid next size (normal)";
goto errout;
    }
```

调试用：`    free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);`

然后是前向合并：

```c
/* consolidate backward */
if (!prev_inuse(p)) {
  prevsize = p->prev_size;
  size += prevsize;
  p = chunk_at_offset(p, -((long) prevsize));
  unlink(av, p, bck, fwd);
}
```

后向合并,并把它扔进`Unsorted bin`：有个关于`Unsorted bin`完整性的检查

```c
  if (nextchunk != av->top) {
    /* get and clear inuse bit */
    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

    /* consolidate forward */
    if (!nextinuse) {
unlink(av, nextchunk, bck, fwd);
size += nextsize;
    } else
clear_inuse_bit_at_offset(nextchunk, 0);//清除nextchunk 的 next chunk的 inuse标志

    /*
Place the chunk in unsorted chunk list. Chunks are
not placed into regular bins until after they have
been given one chance to be used in malloc.
    */

    bck = unsorted_chunks(av);
    fwd = bck->fd;
    if (__glibc_unlikely (fwd->bk != bck))
{
  errstr = "free(): corrupted unsorted chunks";
  goto errout;
}
    p->fd = fwd;
    p->bk = bck;
    if (!in_smallbin_range(size))
{
  p->fd_nextsize = NULL;
  p->bk_nextsize = NULL;
}
    bck->fd = p;
    fwd->bk = p;

    set_head(p, size | PREV_INUSE);
    set_foot(p, size);

    check_free_chunk(av, p);
  }
```

接下来是将`chunk`放进`av->top`。

```c
/*
  If the chunk borders the current high end of memory,
  consolidate into top
*/

else {
  size += nextsize;
  set_head(p, size | PREV_INUSE);
  av->top = p;
  check_chunk(av, p);
}
```

##### 关于更大一点的`chunk`和`mmap`部分,🐦

最后的部分不是很关心，以后来挖坟。

```c
#define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)
```

```c
    /*
      If freeing a large space, consolidate possibly-surrounding
      chunks. Then, if the total unused topmost memory exceeds trim
      threshold, ask malloc_trim to reduce top.
      Unless max_fast is 0, we don't know if there are fastbins
      bordering top, so we cannot tell for sure whether threshold
      has been reached unless fastbins are consolidated.  But we
      don't want to consolidate on each free.  As a compromise,
      consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
      is reached.
    */

    if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
      if (have_fastchunks(av))
	malloc_consolidate(av);

      if (av == &main_arena) {
#ifndef MORECORE_CANNOT_TRIM
	if ((unsigned long)(chunksize(av->top)) >=
	    (unsigned long)(mp_.trim_threshold))
	  systrim(mp_.top_pad, av);
#endif
      } else {
	/* Always try heap_trim(), even if the top chunk is not
	   large, because the corresponding heap might go away.  */
	heap_info *heap = heap_for_ptr(top(av));

	assert(heap->ar_ptr == av);
	heap_trim(heap, mp_.top_pad);
      }
    }

    if (! have_lock) {
      assert (locked);
      (void)mutex_unlock(&av->mutex);
    }
```

#### 总结

`house of spirit`里面伪造`chunk`的时候，

1. Fastbin范围内需要伪造`next chunk`的`size`
2. 大一点的`chunk`需要额外伪造两个`chunk`。