# GLIBC malloc.c free()

> Glibc-2.23

**å‡½æ•°åˆ—è¡¨ï¼š**

1. `malloc(size_t n);`
2. `free(void* p);       `(å½“å‰ page) 
3. `calloc(size_t n_elements, size_t element_size);`
4. `realloc(void* p, size_t n);        `
5. `memalign(size_t alignment, size_t n);   `     
6. `valloc(size_t n);        `
7. `mallinfo();     `
8. `mallopt(int parameter_number, int parameter_value)`

#### `__libc_free()`

```c
void
__libc_free (void *mem)
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    }

  if (mem == 0)                              /* free(0) has no effect */
    return;

  p = mem2chunk (mem);

  if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
      /* see if the dynamic brk/mmap threshold needs adjusting */
      if (!mp_.no_dyn_threshold
          && p->size > mp_.mmap_threshold
          && p->size <= DEFAULT_MMAP_THRESHOLD_MAX)
        {
          mp_.mmap_threshold = chunksize (p);
          mp_.trim_threshold = 2 * mp_.mmap_threshold;
          LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                      mp_.mmap_threshold, mp_.trim_threshold);
        }
      munmap_chunk (p);
      return;
    }

  ar_ptr = arena_for_chunk (p);
  _int_free (ar_ptr, p, 0);
}
libc_hidden_def (__libc_free)
```

é¦–å…ˆï¼Œä¼šæ£€æŸ¥`__free_hook()`æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨ï¼Œç›´æ¥æ‰§è¡Œ`__free_hook()`ã€‚

```c
#ifndef RETURN_ADDRESS
#define RETURN_ADDRESS(X_) (NULL)
#endif
```

æ¥ä¸‹æ¥æ£€æµ‹`free(NULL)`ï¼Œä¸æ“ä½œï¼Œç›´æ¥é€€å‡ºå‡½æ•°ã€‚

ç„¶å` p = mem2chunk (mem);`è·å–`chunk`çš„å¸¦ä¸Š`header`çš„åœ°å€ã€‚

ç„¶åçš„ä¸€æ®µä¼šæ£€æµ‹`chunk`çš„`size`å­—æ®µçš„`IS_MMAPPED`ä½ï¼Œå¦‚æœæ˜¯`mmap`ç”³è¯·çš„å†…å­˜ï¼Œé‚£ä¹ˆç›´æ¥é‡Šæ”¾ï¼Œï¼ˆæš‚æ—¶ä¸å…³æ³¨ï¼‰

ç„¶åå°±åˆ°äº†æˆ‘ç›®å‰çœŸæ­£å…³æ³¨çš„åœ°æ–¹äº†ã€‚

```c
//ç›´æ¥è¿”å› main_arena
[line 1] ar_ptr = arena_for_chunk (p); 

#define arena_for_chunk(ptr) (chunk_non_main_arena (ptr) ? heap_for_ptr (ptr)->ar_ptr : &main_arena)

//è°ƒç”¨_int_free()
[line 2] _int_free (ar_ptr, p, 0);
```

#### ` _int_free()`

å…ˆæŠŠ`main_arena`å†é‡å¤ä¸€ä¸‹å§ï¼Œä¸‡ä¸€ç”¨åˆ°å‘¢ã€‚

```c
struct malloc_state
{
  /* Serialize access.  */
  mutex_t mutex;

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

ç„¶åçœ‹å‡½æ•°çš„å¼€å¤´çš„å˜é‡

```c
static void
_int_free (mstate av, mchunkptr p, int have_lock)
{
  INTERNAL_SIZE_T size;        /* its size */
  mfastbinptr *fb;             /* associated fastbin */ // fastbin
  mchunkptr nextchunk;         /* next contiguous chunk */ //next chunk(address)
  INTERNAL_SIZE_T nextsize;    /* its size */							 //nextchunk size 
  int nextinuse;               /* true if nextchunk is used */
  INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
  mchunkptr bck;               /* misc temp for linking */
  mchunkptr fwd;               /* misc temp for linking */

  const char *errstr = NULL;
  int locked = 0;

  size = chunksize (p);
  [Â·Â·Â·]
}

/* Get size, ignoring use bits */
#define chunksize(p)         ((p)->size & ~(SIZE_BITS))
```

##### é¿å…ä¸äº†çš„æ£€æŸ¥ 

1. `p`çš„åœ°å€å¯¹é½ã€‚
2. è¦ä¿è¯å¤§å°åˆé€‚ï¼š`size > MINSIZE` 
3. `assert(inuse(p))`ï¼Œä»`next chunk`å¤„æ£€æŸ¥`inuse(p)`æ˜¯å¦å·²ç»è¢«åˆ†é…ï¼Œå¦‚æœæ£€æµ‹åˆ°`chunk`æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œé‚£ä¹ˆå°±æŠ¥é”™äº†ã€‚

é¦–å…ˆä¼šæ£€æŸ¥`p`çš„åœ°å€æ˜¯å¦å¯¹é½ã€‚

```c
  if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0)
      || __builtin_expect (misaligned_chunk (p), 0))
    {
      errstr = "free(): invalid pointer";
    errout:
      if (!have_lock && locked)
        (void) mutex_unlock (&av->mutex);
      malloc_printerr (check_action, errstr, chunk2mem (p), av);
      return;
    }
```

ç„¶åæ£€æŸ¥`p->size`æ˜¯å¦å¯¹é½ï¼Œè€Œä¸”`> MINSIZE`

```c
if (__glibc_unlikely (size < MINSIZE || !aligned_OK (size)))
  {
    errstr = "free(): invalid size";
    goto errout;
  }
```

ç„¶åä»`next chunk`å¤„æ£€æŸ¥`inuse(p)`æ˜¯å¦å·²ç»è¢«åˆ†é…ï¼Œå¦‚æœæ£€æµ‹åˆ°`chunk`æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œé‚£ä¹ˆå°±æŠ¥é”™äº†ã€‚

```c
# define check_inuse_chunk(A, P)        do_check_inuse_chunk (A, P)

#define inuse(p)							      \
  ((((mchunkptr) (((char *) (p)) + ((p)->size & ~SIZE_BITS)))->size) & PREV_INUSE)

static void
do_check_inuse_chunk (mstate av, mchunkptr p)
{
  mchunkptr next;

  do_check_chunk (av, p);

  if (chunk_is_mmapped (p))
    return; /* mmapped chunks have no next/prev */

  /* Check whether it claims to be in use ... */
  assert (inuse (p));

  next = next_chunk (p);

  /* ... and is surrounded by OK chunks.
     Since more things can be checked with free chunks than inuse ones,
     if an inuse chunk borders them and debug is on, it's worth doing them.
   */
  if (!prev_inuse (p))
    {
      /* Note that we cannot even look at prev unless it is not inuse */
      mchunkptr prv = prev_chunk (p);
      assert (next_chunk (prv) == p);
      do_check_free_chunk (av, prv);
    }

  if (next == av->top)
    {
      assert (prev_inuse (next));
      assert (chunksize (next) >= MINSIZE);
    }
  else if (!inuse (next))
    do_check_free_chunk (av, next);
}
```

##### Fastbin && æ£€æŸ¥

1. é¦–å…ˆä¼šæ£€æŸ¥`chunk`çš„å¤§å°åˆé€‚ï¼Œå¹¶ä¸”`next chunk`å¤§å°åˆé€‚
2. ç„¶åæ£€æŸ¥Double Freeï¼Œæ£€æŸ¥`fastbin`é‡Œé¢çš„ç¬¬ä¸€ä¸ª`chunk`æ˜¯å¦æ˜¯`p`ã€‚

æ¥ä¸‹æ¥æ˜¯åœ¨Fastbinçš„æ“ä½œï¼Œå¦‚æœ`chunk`åœ¨`Fastbin`èŒƒå›´å†…:

é¦–å…ˆä¼šæ£€æŸ¥`chunk`çš„å¤§å°åˆé€‚ï¼Œå¹¶ä¸”`next chunk`å¤§å°åˆé€‚

```c
/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```

æœ‰ä¸€éƒ¨åˆ†æš‚æ—¶ä¸ç”šå…³ç³»ï¼š

```c
	/* We might not have a lock at this point and concurrent modifications
	   of system_mem might have let to a false positive.  Redo the test
	   after getting the lock.  */
      //lock
	if (have_lock
	    || ({ assert (locked == 0);
		  mutex_lock(&av->mutex);
		  locked = 1;
		  chunk_at_offset (p, size)->size <= 2 * SIZE_SZ
		    || chunksize (chunk_at_offset (p, size)) >= av->system_mem;
	      }))
	  {
	    errstr = "free(): invalid next size (fast)";
	    goto errout;
	  }
	if (! have_lock)
	  {
	    (void)mutex_unlock(&av->mutex);
	    locked = 0;
	  }
      }

    //è°ƒè¯•ç”¨ã€‚
    free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);
```

æ¥ä¸‹æ¥å°†`main_arena`ä¸­`flag`æ ‡è®°ä¸ºæœ‰`fastbin chunk`

```c
set_fastchunks(av);
```

ç„¶åå–å¾—ç›¸åº”çš„`fastbin`çš„ç´¢å¼•`idx`å’ŒæŒ‡é’ˆã€‚

```c
#define fastbin(ar_ptr,idx) ((ar_ptr)->fastbinsY[idx])

unsigned int idx = fastbin_index(size);
fb = &fastbin (av, idx);
```

ç„¶åæ£€æŸ¥Double Freeï¼Œæ£€æŸ¥`fastbin`é‡Œé¢çš„ç¬¬ä¸€ä¸ª`chunk`æ˜¯å¦æ˜¯`p`ï¼Œç„¶åé€šè¿‡CASæ“ä½œå°†`chunk`æ·»åŠ åˆ°`fastbin`ä¸­ã€‚

```c
  /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
  mchunkptr old = *fb, old2;
  unsigned int old_idx = ~0u;
  do
    {
/* Check that the top of the bin is not the record we are going to add
   (i.e., double free).  */
if (__builtin_expect (old == p, 0))
  {
    errstr = "double free or corruption (fasttop)";
    goto errout;
  }
/* Check that size of fastbin chunk at the top is the same as
   size of the chunk that we are adding.  We can dereference OLD
   only if we have the lock, otherwise it might have already been
   deallocated.  See use of OLD_IDX below for the actual check.  */
if (have_lock && old != NULL)
  old_idx = fastbin_index(chunksize(old));
p->fd = old2 = old;
    }
  while ((old = catomic_compare_and_exchange_val_rel (fb, p, old2)) != old2);

  if (have_lock && old != NULL && __builtin_expect (old_idx != idx, 0))
    {
errstr = "invalid fastbin entry (free)";
goto errout;
    }
}
```

åœ¨ç¡®å®šä¸æ˜¯`fastbin`èŒƒå›´å†…ï¼Œè¿›å…¥`else`ä»£ç æ®µï¼š

##### éFastbin && æ£€æŸ¥

1. æ£€æŸ¥`chunk`æ˜¯å¦æ˜¯`mmap`å¾—åˆ°çš„
2. æ£€æŸ¥`chunk`ä¸æ˜¯`av->top`
3. ç„¶åæ£€æŸ¥`chunk`ä¸èƒ½è¶…è¿‡`arena`çš„è¾¹ç•Œï¼Œ`nextchunk= av->top + chunksize(av->top)`
4. `nextchunk->size` :åœ¨`netx chunk->size`æ£€æŸ¥`p`æ˜¯å¦æ²¡æœ‰è¢«æ ‡è®°ä¸ºå·²è¢«åˆ†é…
5. `nextchunk->size` :æ£€æŸ¥`next chunk`å¤§å°åœ¨åˆç†èŒƒå›´å†…
6. åå‘åˆå¹¶,å¹¶æŠŠæ–°çš„`chunk`æ‰”è¿›`Unsorted bin`è¿™é‡Œæœ‰ä¸ªå…³äº`Unsorted bin`å®Œæ•´æ€§çš„æ£€æŸ¥ã€‚

æ£€æŸ¥`chunk`æ˜¯å¦æ˜¯`mmap`å¾—åˆ°çš„ï¼š

```c
/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)

else if (!chunk_is_mmapped(p)) {
  if (! have_lock) {
    (void)mutex_lock(&av->mutex);
    locked = 1;
  }
```

ç„¶åè·å–`next chunk`çš„åœ°å€ï¼š

```c
nextchunk = chunk_at_offset(p, size);
```

æ£€æŸ¥`chunk`ä¸æ˜¯`av->top`:

```c
  /* Lightweight tests: check whether the block is already the
     top block.  */
  if (__glibc_unlikely (p == av->top))
    {
errstr = "double free or corruption (top)";
goto errout;
    }
```

ç„¶åæ£€æŸ¥`chunk`ä¸èƒ½è¶…è¿‡`arena`çš„è¾¹ç•Œ

```c
  /* Or whether the next chunk is beyond the boundaries of the arena.  */
  if (__builtin_expect (contiguous (av)
      && (char *) nextchunk
      >= ((char *) av->top + chunksize(av->top)), 0))
    {
errstr = "double free or corruption (out)";
goto errout;
    }
```

åœ¨`netx chunk->size`æ£€æŸ¥`p`æ˜¯å¦æ²¡æœ‰è¢«æ ‡è®°ä¸ºå·²è¢«åˆ†é…

```c
  if (__glibc_unlikely (!prev_inuse(nextchunk)))
    {
errstr = "double free or corruption (!prev)";
goto errout;
    }
```

æ£€æŸ¥`next chunk`å¤§å°åœ¨åˆç†èŒƒå›´å†…ï¼š

```c
  nextsize = chunksize(nextchunk);
  if (__builtin_expect (nextchunk->size <= 2 * SIZE_SZ, 0)
|| __builtin_expect (nextsize >= av->system_mem, 0))
    {
errstr = "free(): invalid next size (normal)";
goto errout;
    }
```

è°ƒè¯•ç”¨ï¼š`    free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);`

ç„¶åæ˜¯å‰å‘åˆå¹¶ï¼š

```c
/* consolidate backward */
if (!prev_inuse(p)) {
  prevsize = p->prev_size;
  size += prevsize;
  p = chunk_at_offset(p, -((long) prevsize));
  unlink(av, p, bck, fwd);
}
```

åå‘åˆå¹¶,å¹¶æŠŠå®ƒæ‰”è¿›`Unsorted bin`ï¼šæœ‰ä¸ªå…³äº`Unsorted bin`å®Œæ•´æ€§çš„æ£€æŸ¥

```c
  if (nextchunk != av->top) {
    /* get and clear inuse bit */
    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

    /* consolidate forward */
    if (!nextinuse) {
unlink(av, nextchunk, bck, fwd);
size += nextsize;
    } else
clear_inuse_bit_at_offset(nextchunk, 0);//æ¸…é™¤nextchunk çš„ next chunkçš„ inuseæ ‡å¿—

    /*
Place the chunk in unsorted chunk list. Chunks are
not placed into regular bins until after they have
been given one chance to be used in malloc.
    */

    bck = unsorted_chunks(av);
    fwd = bck->fd;
    if (__glibc_unlikely (fwd->bk != bck))
{
  errstr = "free(): corrupted unsorted chunks";
  goto errout;
}
    p->fd = fwd;
    p->bk = bck;
    if (!in_smallbin_range(size))
{
  p->fd_nextsize = NULL;
  p->bk_nextsize = NULL;
}
    bck->fd = p;
    fwd->bk = p;

    set_head(p, size | PREV_INUSE);
    set_foot(p, size);

    check_free_chunk(av, p);
  }
```

æ¥ä¸‹æ¥æ˜¯å°†`chunk`æ”¾è¿›`av->top`ã€‚

```c
/*
  If the chunk borders the current high end of memory,
  consolidate into top
*/

else {
  size += nextsize;
  set_head(p, size | PREV_INUSE);
  av->top = p;
  check_chunk(av, p);
}
```

##### å…³äºæ›´å¤§ä¸€ç‚¹çš„`chunk`å’Œ`mmap`éƒ¨åˆ†,ğŸ¦

æœ€åçš„éƒ¨åˆ†ä¸æ˜¯å¾ˆå…³å¿ƒï¼Œä»¥åæ¥æŒ–åŸã€‚

```c
#define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)
```

```c
    /*
      If freeing a large space, consolidate possibly-surrounding
      chunks. Then, if the total unused topmost memory exceeds trim
      threshold, ask malloc_trim to reduce top.
      Unless max_fast is 0, we don't know if there are fastbins
      bordering top, so we cannot tell for sure whether threshold
      has been reached unless fastbins are consolidated.  But we
      don't want to consolidate on each free.  As a compromise,
      consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
      is reached.
    */

    if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
      if (have_fastchunks(av))
	malloc_consolidate(av);

      if (av == &main_arena) {
#ifndef MORECORE_CANNOT_TRIM
	if ((unsigned long)(chunksize(av->top)) >=
	    (unsigned long)(mp_.trim_threshold))
	  systrim(mp_.top_pad, av);
#endif
      } else {
	/* Always try heap_trim(), even if the top chunk is not
	   large, because the corresponding heap might go away.  */
	heap_info *heap = heap_for_ptr(top(av));

	assert(heap->ar_ptr == av);
	heap_trim(heap, mp_.top_pad);
      }
    }

    if (! have_lock) {
      assert (locked);
      (void)mutex_unlock(&av->mutex);
    }
```

#### æ€»ç»“

`house of spirit`é‡Œé¢ä¼ªé€ `chunk`çš„æ—¶å€™ï¼Œ

1. FastbinèŒƒå›´å†…éœ€è¦ä¼ªé€ `next chunk`çš„`size`
2. å¤§ä¸€ç‚¹çš„`chunk`éœ€è¦é¢å¤–ä¼ªé€ ä¸¤ä¸ª`chunk`ã€‚