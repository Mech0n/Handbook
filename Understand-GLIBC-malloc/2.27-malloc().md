# GLIBC-2.27-malloc()

> GLIBC-2.27

è¿™éƒ¨åˆ†ä»‹ç»2.27ç‰ˆæœ¬ä¸‹çš„malloc()ï¼Œé¡ºä¾¿å’Œ2.23ç‰ˆæœ¬çš„åšä¸€ä¸‹å¯¹æ¯”ã€‚

#### `_libc_malloc()`

```c
void *
__libc_malloc (size_t bytes)
{
  mstate ar_ptr;
  void *victim;

  void *(*hook) (size_t, const void *)
    = atomic_forced_read (__malloc_hook);
  if (__builtin_expect (hook != NULL, 0))
    return (*hook)(bytes, RETURN_ADDRESS (0));
//[Tcache-begin]
  #if USE_TCACHE
  /* int_free also calls request2size, be careful to not pad twice.  */
  size_t tbytes;
  checked_request2size (bytes, tbytes);
  size_t tc_idx = csize2tidx (tbytes);

  MAYBE_INIT_TCACHE ();

  DIAG_PUSH_NEEDS_COMMENT;
  if (tc_idx < mp_.tcache_bins
      /*&& tc_idx < TCACHE_MAX_BINS*/ /* to appease gcc */
      && tcache
      && tcache->entries[tc_idx] != NULL)
    {
      return tcache_get (tc_idx);
    }
  DIAG_POP_NEEDS_COMMENT;
#endif

  if (SINGLE_THREAD_P)
    {
      victim = _int_malloc (&main_arena, bytes);
      assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
	      &main_arena == arena_for_chunk (mem2chunk (victim)));
      return victim;
    }
//[Tcache-End]
  arena_get (ar_ptr, bytes);

  victim = _int_malloc (ar_ptr, bytes);
  /* Retry with another arena only if we were able to find a usable arena
     before.  */
  if (!victim && ar_ptr != NULL)
    {
      LIBC_PROBE (memory_malloc_retry, 1, bytes);
      ar_ptr = arena_get_retry (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
    }

  if (ar_ptr != NULL)
    __libc_lock_unlock (ar_ptr->mutex);

  assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
          ar_ptr == arena_for_chunk (mem2chunk (victim)));
  return victim;
}
libc_hidden_def (__libc_malloc)
```

**å¯ä»¥çœ‹åˆ°ï¼Œç›¸è¾ƒ2.23çš„`_libc_malloc()`,åœ¨`__malloc_hook()`ä¹‹åï¼Œå¢åŠ äº†é€‰ä¸­çš„`Tcache`éƒ¨åˆ†ã€‚**

é¦–å…ˆè·å¾—ä¸€ä¸ªæŠŠè¯·æ±‚çš„å¤§å°è½¬æ¢ä¸ºæ ‡å‡†çš„`size`å¤§å°ï¼Œå¹¶å¾—åˆ°ç›¸å…³çš„`tcachebin`çš„`idx`ã€‚

```c
/* pad request bytes into a usable size -- internal version */

#define request2size(req)                                         \
  (((req) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
   MINSIZE :                                                      \
   ((req) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)

/* Same, except also perform an argument and result check.  First, we check
   that the padding done by request2size didn't result in an integer
   overflow.  Then we check (using REQUEST_OUT_OF_RANGE) that the resulting
   size isn't so large that a later alignment would lead to another integer
   overflow.  */
#define checked_request2size(req, sz) \
({				    \
  (sz) = request2size (req);	    \
  if (((sz) < (req))		    \
      || REQUEST_OUT_OF_RANGE (sz)) \
    {				    \
      __set_errno (ENOMEM);	    \
      return 0;			    \
    }				    \
})

/* When "x" is from chunksize().  */
# define csize2tidx(x) (((x) - MINSIZE + MALLOC_ALIGNMENT - 1) / MALLOC_ALIGNMENT)

size_t tbytes;
checked_request2size (bytes, tbytes);
size_t tc_idx = csize2tidx (tbytes);
```

æ¥ä¸‹æ¥ä¼šè°ƒç”¨`tcache_init()`æ¥åˆå§‹åŒ–`tcachebin`ã€‚

```c
# define MAYBE_INIT_TCACHE() \
  if (__glibc_unlikely (tcache == NULL)) \
    tcache_init();

MAYBE_INIT_TCACHE ();
```

å±•å¼€çœ‹ä¸€ä¸‹`tcache_init()`ç›¸å…³éƒ¨åˆ†ï¼š

####  `tcache_init()`

**è¿™ä¸ªå‡½æ•°å°±æ˜¯ç”³è¯·ä¸€ä¸ª`chunk`æ¥ç®¡ç†`Tcache`ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨åˆ°çš„ç¬¬ä¸€ä¸ª`chunk`ï¼Œåº”è¯¥æ˜¯`tcache bin`**

**è€Œç®¡ç†å…¶ä»–`bin`çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡`main_state`ã€‚**

```c
# define TCACHE_MAX_BINS		64

/* We overlay this structure on the user-data portion of a chunk when
   the chunk is stored in the per-thread cache.  */
typedef struct tcache_entry
{
  struct tcache_entry *next;
} tcache_entry;

/* There is one of these for each thread, which contains the
   per-thread cache (hence "tcache_perthread_struct").  Keeping
   overall size low is mildly important.  Note that COUNTS and ENTRIES
   are redundant (we could have just counted the linked list each
   time), this is for performance reasons.  */
typedef struct tcache_perthread_struct
{
  char counts[TCACHE_MAX_BINS];
  tcache_entry *entries[TCACHE_MAX_BINS];
} tcache_perthread_struct;

static __thread bool tcache_shutting_down = false;

//--------------------------åˆ†å‰²çº¿----------------------

static void
tcache_init(void)
{
  mstate ar_ptr;
  void *victim = 0;
  const size_t bytes = sizeof (tcache_perthread_struct);

  if (tcache_shutting_down)
    return;

  arena_get (ar_ptr, bytes);
  victim = _int_malloc (ar_ptr, bytes);
  if (!victim && ar_ptr != NULL)
    {
      ar_ptr = arena_get_retry (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
    }


  if (ar_ptr != NULL)
//#define __libc_lock_lock(NAME) __mutex_lock (&(NAME)) è·Ÿ2.23çš„å‡½æ•°å°±ä¸€æ ·äº†ã€‚
    __libc_lock_unlock (ar_ptr->mutex);

  /* In a low memory situation, we may not be able to allocate memory
     - in which case, we just keep trying later.  However, we
     typically do this very early, so either there is sufficient
     memory, or there isn't enough memory to do non-trivial
     allocations anyway.  */
  if (victim)
    {
      tcache = (tcache_perthread_struct *) victim;
      memset (tcache, 0, sizeof (tcache_perthread_struct));
    }
}
```

å¯ä»¥çœ‹åˆ°ï¼Œ`tcache_init`é¦–å…ˆè°ƒç”¨`_int_malloc()`ç”³è¯·äº†ä¸€ä¸ª`0x240 + 0x10`çš„`chunk`æ¥å­˜æ”¾`tcache_perthread_struct`ç»“æ„ä½“ï¼Œä½œä¸º`tcache bin`çš„ä½ç½®ã€‚

ç„¶åä¼šæ£€æŸ¥`tcache bin`é‡Œæœ‰æ²¡æœ‰åˆé€‚çš„`chunk`ï¼Œå¦‚æœæœ‰å°±ç”¨åˆ°äº†`tcache`çš„ç›¸å…³å‡½æ•°äº†

```c
if (tc_idx < mp_.tcache_bins
    /*&& tc_idx < TCACHE_MAX_BINS*/ /* to appease gcc */
    && tcache
    && tcache->entries[tc_idx] != NULL)
  {
    return tcache_get (tc_idx);
  }
```

#### ğŸ†•`tcache_get(idx)` && æ£€æŸ¥

**åˆ°è¿™é‡Œä¹‹å‰ä»€ä¹ˆæ£€æŸ¥éƒ½æ²¡æœ‰ã€‚**

è¿™é‡Œä¸¤ä¸ªæ£€æŸ¥ï¼Œç›¸å½“äºæ²¡æœ‰æ£€æŸ¥ã€‚

1. `assert (tc_idx < TCACHE_MAX_BINS);`æ£€æŸ¥`idx`æ˜¯å¦åˆæ³•ã€‚
2. `assert (tcache->entries[tc_idx] > 0);`ï¼Œ`tcache bin`é‡Œæ˜¯å¦æœ‰ä¸œè¥¿ã€‚

```c
/* Caller must ensure that we know tc_idx is valid and there's
   available chunks to remove.  */
static __always_inline void *
tcache_get (size_t tc_idx)
{
  tcache_entry *e = tcache->entries[tc_idx];
  assert (tc_idx < TCACHE_MAX_BINS);
  assert (tcache->entries[tc_idx] > 0);
  tcache->entries[tc_idx] = e->next;
  --(tcache->counts[tc_idx]);
  return (void *) e;
}
```

æ¥ä¸‹æ¥æ˜¯`arena_get()`ï¼Œæ²¡æœ‰å¤šä½™çš„å˜åŒ–ï¼Œå°±æ˜¯è·å¾—ä¸€ä¸ª`main_state`ã€‚

```c
#define arena_get(ptr, size) do { \
      ptr = thread_arena;						      \
      arena_lock (ptr, size);						      \
  } while (0)

#define arena_lock(ptr, size) do {					      \
      if (ptr)								      \
        __libc_lock_lock (ptr->mutex);					      \
      else								      \
        ptr = arena_get2 ((size), NULL);				      \
  } while (0)
```

ç„¶åè°ƒç”¨`_int_malloc()`

`victim = _int_malloc (ar_ptr, bytes);`

#### `_int_malloc()` 

å…ˆæ”¾ä¸€ä¸‹`main_state`:

```c
struct malloc_state
{
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

ç„¶åçœ‹`_int_malloc()`çš„å˜é‡ï¼Œå¤šäº†å…³äº`tcache bin`çš„ã€‚

```c
#if USE_TCACHE
  size_t tcache_unsorted_count;	    /* count of unsorted chunks processed */
#endif
```

##### ğŸ†•`REMOVE_FB()` 

ç„¶åï¼Œè¿™é‡Œæ–°åŠ ä¸€ä¸ª`REMOVE_FB()`æ¥å–å‡º`Fastbin`çš„`chunk`ã€‚

```c
#define REMOVE_FB(fb, victim, pp)			
  do							
    {							
      victim = pp;					
      if (victim == NULL)				
	break;						
    }							
  while ((pp = catomic_compare_and_exchange_val_acq (fb, victim->fd, victim)) 
	 != victim);					
```

##### `Fastbin` && æ£€æŸ¥

**æ£€æŸ¥ä¸ä¹‹å‰ä¸€æ ·**

ç„¶åè¿›å…¥`fastbin`éƒ¨åˆ†

```c
  if ((unsigned long) (nb) <= (unsigned long) (get_max_fast ()))
    {
      idx = fastbin_index (nb);
      mfastbinptr *fb = &fastbin (av, idx);
      mchunkptr pp;
      victim = *fb;

      if (victim != NULL)
	{
	  if (SINGLE_THREAD_P)
	    *fb = victim->fd;
	  else
	    REMOVE_FB (fb, pp, victim);
	  if (__glibc_likely (victim != NULL))
	    {
	      size_t victim_idx = fastbin_index (chunksize (victim));
	      if (__builtin_expect (victim_idx != idx, 0))
		malloc_printerr ("malloc(): memory corruption (fast)");
	      check_remalloced_chunk (av, victim, nb);
#if USE_TCACHE
	      /* While we're here, if we see other chunks of the same size,
		 stash them in the tcache.  */
	      size_t tc_idx = csize2tidx (nb);
	      if (tcache && tc_idx < mp_.tcache_bins)
		{
		  mchunkptr tc_victim;

		  /* While bin not empty and tcache not full, copy chunks.  */
		  while (tcache->counts[tc_idx] < mp_.tcache_count
			 && (tc_victim = *fb) != NULL)
		    {
		      if (SINGLE_THREAD_P)
			*fb = tc_victim->fd;
		      else
			{
			  REMOVE_FB (fb, pp, tc_victim);
			  if (__glibc_unlikely (tc_victim == NULL))
			    break;
			}
		      tcache_put (tc_victim, tc_idx);
		    }
		}
#endif
	      void *p = chunk2mem (victim);
	      alloc_perturb (p, bytes);
	      return p;
	    }
	}
    }
```

è¿™é‡Œçš„é€»è¾‘ä¸ä¸€æ ·äº†ï¼Œé¦–å…ˆå–å‡ºä¸€ä¸ª`Fastbin`çš„`chunk`ï¼Œç„¶åæŠŠ`Fastbin`å‰©ä½™çš„`chunk`ç»Ÿç»Ÿæ‰”è¿›`tcache bin`é‡Œã€‚

ï¼ˆå½“ç„¶æ»¡äº†å°±ä¸è¡Œäº†ï¼Œ`max == 7`ï¼‰

è¿™é‡Œç”¨åˆ°äº†æ–°çš„å‡½æ•°`tcache_put()`

##### ğŸ†•`tcache_put()` && æ£€æŸ¥

**è¿™é‡Œå¯ä»¥è¯´æ²¡æœ‰æ£€æŸ¥ã€‚**

1. `assert (tc_idx < TCACHE_MAX_BINS);`åªæ£€æŸ¥äº†ä¼ å…¥çš„`idx`ã€‚

```c
static __always_inline void
tcache_put (mchunkptr chunk, size_t tc_idx)
{
  tcache_entry *e = (tcache_entry *) chunk2mem (chunk);
  assert (tc_idx < TCACHE_MAX_BINS);
  e->next = tcache->entries[tc_idx];
  tcache->entries[tc_idx] = e;
  ++(tcache->counts[tc_idx]);
}
```

##### `small bin`&& æ£€æŸ¥

**æ£€æŸ¥ä¸ä¹‹å‰ä¸€æ ·**

```c
  if (in_smallbin_range (nb))
    {
      idx = smallbin_index (nb);
      bin = bin_at (av, idx);

      if ((victim = last (bin)) != bin)
        {
          bck = victim->bk;
	  if (__glibc_unlikely (bck->fd != victim))
	    malloc_printerr ("malloc(): smallbin double linked list corrupted");
          set_inuse_bit_at_offset (victim, nb);
          bin->bk = bck;
          bck->fd = bin;

          if (av != &main_arena)
	    set_non_main_arena (victim);
          check_malloced_chunk (av, victim, nb);
#if USE_TCACHE
	  /* While we're here, if we see other chunks of the same size,
	     stash them in the tcache.  */
	  size_t tc_idx = csize2tidx (nb);
	  if (tcache && tc_idx < mp_.tcache_bins)
	    {
	      mchunkptr tc_victim;

	      /* While bin not empty and tcache not full, copy chunks over.  */
	      while (tcache->counts[tc_idx] < mp_.tcache_count
		     && (tc_victim = last (bin)) != bin)
		{
		  if (tc_victim != 0)
		    {
		      bck = tc_victim->bk;
		      set_inuse_bit_at_offset (tc_victim, nb);
		      if (av != &main_arena)
			set_non_main_arena (tc_victim);
		      bin->bk = bck;
		      bck->fd = bin;

		      tcache_put (tc_victim, tc_idx);
	            }
		}
	    }
#endif
          void *p = chunk2mem (victim);
          alloc_perturb (p, bytes);
          return p;
        }
    }
```

é¦–å…ˆæŒ‰ç…§`idx`å–å‡º`chunk`ï¼Œç„¶åæ£€æŸ¥`small bin`æ˜¯å¦å®Œæ•´ï¼Œç„¶åè®¾ç½®`mmap`ä½ï¼Œæ£€æŸ¥`chunk`æ˜¯å¦æ›¾ç»å·²ç»è¢«åˆ†é…ã€‚

```c
# define check_malloced_chunk(A, P, N)   do_check_malloced_chunk (A, P, N)

check_malloced_chunk (av, victim, nb);
```

ç„¶åè¿›å…¥`tcache bin`éƒ¨åˆ†ï¼Œå°†å…¶ä»–`chunk`æ”¾è¿›`tcache bin`ï¼Œç›´åˆ°`tcache bin`è¢«å¡«æ»¡ã€‚ï¼ˆæ²¡æ£€æŸ¥ğŸ˜“ï¼‰

```c
#if USE_TCACHE
	  /* While we're here, if we see other chunks of the same size,
	     stash them in the tcache.  */
	  size_t tc_idx = csize2tidx (nb);
	  if (tcache && tc_idx < mp_.tcache_bins)
	    {
	      mchunkptr tc_victim;

	      /* While bin not empty and tcache not full, copy chunks over.  */
	      while (tcache->counts[tc_idx] < mp_.tcache_count
		     && (tc_victim = last (bin)) != bin)
		{
		  if (tc_victim != 0)
		    {
		      bck = tc_victim->bk;
		      set_inuse_bit_at_offset (tc_victim, nb);
		      if (av != &main_arena)
			set_non_main_arena (tc_victim);
		      bin->bk = bck;
		      bck->fd = bin;

		      tcache_put (tc_victim, tc_idx);
	            }
		}
	    }
#endif
```

##### `Unsorted bin` && æ£€æŸ¥

ç„¶åæ˜¯`Unsorted bin`ã€‚é¦–å…ˆæ˜¯`malloc_consolidate()`ã€‚å°±æ˜¯å°†åˆå¹¶åçš„`fastbin`é‡Œçš„`chunk`èƒ½åˆå¹¶çš„åˆå¹¶ï¼Œç„¶åä¸¢è¿›`Unsorted bin`ã€‚

**ç„¶åæ˜¯`Tcache bin`çš„æ“ä½œï¼Œæš‚æ—¶è¿˜æ˜¯ä¸çŸ¥é“å¹²å•¥çš„ï¼Œå…ˆæ”¾ä¸€æ”¾ã€‚ï¼ˆ**

```c
#if USE_TCACHE
  INTERNAL_SIZE_T tcache_nb = 0;
  size_t tc_idx = csize2tidx (nb);
  if (tcache && tc_idx < mp_.tcache_bins)
    tcache_nb = nb;
  int return_cached = 0;

  tcache_unsorted_count = 0;
#endif
```

ç„¶åè·Ÿä¹‹å‰ä¸€æ ·ä¼šå…ˆå–å‡º`chunk`ã€‚

å½“å–å‡ºçš„`chunk`çš„`size`æ­£å¥½æ»¡è¶³éœ€æ±‚ï¼Œæœ‰å˜åŒ–äº†ï¼Œå¦‚æœèƒ½æŠŠ`chunk`å°±ä¼šå…ˆæ”¾è¿›`Tcache bin`ï¼Œç„¶åå°†`return_cached`ç½®`1`ï¼Œé‡æ–°è¿›å…¥å¾ªç¯ï¼Œå¦åˆ™å°±è¿”å›`chunk`ç»™æˆ‘ä»¬äº†ã€‚

```c
          if (size == nb)
            {
              set_inuse_bit_at_offset (victim, size);
              if (av != &main_arena)
		set_non_main_arena (victim);
#if USE_TCACHE
	      /* Fill cache first, return to user only if cache fills.
		 We may return one of these chunks later.  */
	      if (tcache_nb
		  && tcache->counts[tc_idx] < mp_.tcache_count)
		{
		  tcache_put (victim, tc_idx);
		  return_cached = 1;
		  continue;
		}
	      else
		{
#endif
              check_malloced_chunk (av, victim, nb);
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
#if USE_TCACHE
		}
#endif
```

å¦‚æœ`chunk`ä¸æ»¡è¶³æˆ‘ä»¬éœ€è¦çš„å¤§å°ã€‚å°±ä¼šå¾€ä¸‹èµ°ã€‚åˆ¤å®šå–å‡ºçš„`chunk`çš„`size`æ˜¯å¦æ˜¯`small bin`èŒƒå›´å†…ï¼Œå¦‚æœåœ¨ï¼Œå°±ç›´æ¥ä¸¢è¿›`small bin`ã€‚

å¦åˆ™ä¼šä¸¢è¿›`largebin`ã€‚

```c
          /* place chunk in bin */

          if (in_smallbin_range (size))
            {
              victim_index = smallbin_index (size);
              bck = bin_at (av, victim_index);
              fwd = bck->fd;
            }
          else
            {
              victim_index = largebin_index (size);
              bck = bin_at (av, victim_index);
              fwd = bck->fd;

              /* maintain large bins in sorted order */
              if (fwd != bck)
                {
                  /* Or with inuse bit to speed comparisons */
                  size |= PREV_INUSE;
                  /* if smaller than smallest, bypass loop below */
                  assert (chunk_main_arena (bck->bk));
                  if ((unsigned long) (size)
		      < (unsigned long) chunksize_nomask (bck->bk))
                    {
                      fwd = bck;
                      bck = bck->bk;

                      victim->fd_nextsize = fwd->fd;
                      victim->bk_nextsize = fwd->fd->bk_nextsize;
                      fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
                    }
                  else
                    {
                      assert (chunk_main_arena (fwd));
                      while ((unsigned long) size < chunksize_nomask (fwd))
                        {
                          fwd = fwd->fd_nextsize;
			  assert (chunk_main_arena (fwd));
                        }

                      if ((unsigned long) size
			  == (unsigned long) chunksize_nomask (fwd))
                        /* Always insert in the second position.  */
                        fwd = fwd->fd;
                      else
                        {
                          victim->fd_nextsize = fwd;
                          victim->bk_nextsize = fwd->bk_nextsize;
                          fwd->bk_nextsize = victim;
                          victim->bk_nextsize->fd_nextsize = victim;
                        }
                      bck = fwd->bk;
                    }
                }
              else
                victim->fd_nextsize = victim->bk_nextsize = victim;
            }

          mark_bin (av, victim_index);
          victim->bk = bck;
          victim->fd = fwd;
          fwd->bk = victim;
          bck->fd = victim;

#if USE_TCACHE
      /* If we've processed as many chunks as we're allowed while
	 filling the cache, return one of the cached ones.  */
      ++tcache_unsorted_count;
      if (return_cached
	  && mp_.tcache_unsorted_limit > 0
	  && tcache_unsorted_count > mp_.tcache_unsorted_limit)
	{
	  return tcache_get (tc_idx);
	}
#endif

#define MAX_ITERS       10000
          if (++iters >= MAX_ITERS)
            break;
        }

#if USE_TCACHE
      /* If all the small chunks we found ended up cached, return one now.  */
      if (return_cached)
	{
	  return tcache_get (tc_idx);
	}
#endif
```

ä½†æ˜¯è¿™é‡Œè¿˜æœ‰`tcache bin`çš„æ“ä½œï¼Œéå† `unsorted bin` çš„æœ€åï¼Œä¼šæ ¹æ® `return_cached` åˆ¤æ–­æ˜¯å¦æœ‰ å¤§å°é€‚é…çš„ `unsorted bin` è¿›å…¥äº† `tcache` ï¼Œ `mp_.tcache_unsorted_limit` é»˜è®¤ä¸º `0` ï¼Œæ‰€ä»¥ä¸ä¼šè¿›å…¥åˆ†æ”¯ã€‚

ç„¶åï¼Œå¦‚æœä¹‹å‰é‚£ä¸ª`chunk`åˆšå¥½æ»¡è¶³`size`ï¼Œå°±ä¼šå–å‡ºæ¥è¿”å›ã€‚

##### `Largebin`&&æ£€æŸ¥

æ¥ä¸‹æ¥çœ‹`Large bin`éƒ¨åˆ†çš„æ“ä½œã€‚

```c

```

