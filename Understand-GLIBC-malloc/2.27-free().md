# 2.27 free()函数

>  2.27下的free()函数。

首先进入`_libc_free()`

### `__libc_free()`

1. 首先会检查`__free_hook()`是否为空。
2. 然后检查`free(0);`
3. 然后会检查`mmap`位.

```c
void
__libc_free (void *mem)
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    }

  if (mem == 0)                              /* free(0) has no effect */
    return;
```

第一部分显然是尝试调用`__free_hook()`函数。

然后检查是否是`free(0);`，如果是就结束函数。

```c
p = mem2chunk (mem);

if (chunk_is_mmapped (p))                       /* release mmapped memory. */
  {
    /* See if the dynamic brk/mmap threshold needs adjusting.
 Dumped fake mmapped chunks do not affect the threshold.  */
    if (!mp_.no_dyn_threshold
        && chunksize_nomask (p) > mp_.mmap_threshold
        && chunksize_nomask (p) <= DEFAULT_MMAP_THRESHOLD_MAX
  && !DUMPED_MAIN_ARENA_CHUNK (p))
      {
        mp_.mmap_threshold = chunksize (p);
        mp_.trim_threshold = 2 * mp_.mmap_threshold;
        LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                    mp_.mmap_threshold, mp_.trim_threshold);
      }
    munmap_chunk (p);
    return;
  }
```

然后转换`free()`的地址为`chunk`的实际地址，检查是否是`mmap()`出来的，如果是，直接释放。

然后会初始化`tcache bin`。

```c
# define MAYBE_INIT_TCACHE() \
  if (__glibc_unlikely (tcache == NULL)) \
    tcache_init();

[line 2921] static __thread tcache_perthread_struct *tcache = NULL; // 全局变量

MAYBE_INIT_TCACHE ();
```

这部分可以参考我之前写的[2.27-malloc()](./2.27-malloc().md#tcache_init)

>  **这个函数就是申请一个`0x240 + 0x10`的`chunk`来管理`Tcache`，所以我们用到的第一个`chunk`，应该是`tcache bin`而管理其他`bin`的是一个全局变量`main_state`。**

最后到了我们真正关注的部分：

```c
ar_ptr = arena_for_chunk (p);
_int_free (ar_ptr, p, 0);
```

### `__int_free()`

首先看声明变量，并复习`malloc_state`，**多了个`have_fastchunks`变量。**

```c
struct malloc_state
{
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks; [🆕]

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

然后是函数开头的变量：

```c
INTERNAL_SIZE_T size;        /* its size */
mfastbinptr *fb;             /* associated fastbin */
mchunkptr nextchunk;         /* next contiguous chunk */
INTERNAL_SIZE_T nextsize;    /* its size */
int nextinuse;               /* true if nextchunk is used */
INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
mchunkptr bck;               /* misc temp for linking */
mchunkptr fwd;               /* misc temp for linking */
```

没什么变化。

#### 避免不了的检查

1. 检查`p`的位置是否内存对齐，且大小合适，另外就是`size`的大小必须对齐且大于最小块大小。
2. 检查`p`是否是正在使用的`chunk`。这里会检查`p`前一个`chunk`的大小，是否与`p`连接，和后一个`chunk`的`pre_inuse`位和后一个个`chunk`的大小是否大于最小的`chunk`大小。

接下来开始第一个检查：

检查`p`的位置是否内存对齐，且大小合适，另外就是`size`的大小必须对齐且大于最小块大小。

```c
size = chunksize (p);

/* Little security check which won't hurt performance: the
   allocator never wrapps around at the end of the address space.
   Therefore we can exclude some size values which might appear
   here by accident or by "design" from some intruder.  */
if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0)
    || __builtin_expect (misaligned_chunk (p), 0))
  malloc_printerr ("free(): invalid pointer");
/* We know that each chunk is at least MINSIZE bytes in size or a
   multiple of MALLOC_ALIGNMENT.  */
if (__glibc_unlikely (size < MINSIZE || !aligned_OK (size)))
  malloc_printerr ("free(): invalid size");
```

然后是检查`p`是否是正在使用的`chunk`，这里会检查`p`前一个`chunk`的大小，是否与`p`连接，和后一个`chunk`的`pre_inuse`位和后一个个`chunk`的大小是否大于最小的`chunk`大小。

```c
check_inuse_chunk(av, p);

# define check_inuse_chunk(A, P)        do_check_inuse_chunk (A, P)

#define inuse(p)							      \
  ((((mchunkptr) (((char *) (p)) + ((p)->size & ~SIZE_BITS)))->size) & PREV_INUSE)

#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))

static void
do_check_inuse_chunk (mstate av, mchunkptr p)
{
  mchunkptr next;

  do_check_chunk (av, p);

  if (chunk_is_mmapped (p))
    return; /* mmapped chunks have no next/prev */

  /* Check whether it claims to be in use ... */
  assert (inuse (p));

  next = next_chunk (p);

  /* ... and is surrounded by OK chunks.
     Since more things can be checked with free chunks than inuse ones,
     if an inuse chunk borders them and debug is on, it's worth doing them.
   */
  if (!prev_inuse (p))
    {
      /* Note that we cannot even look at prev unless it is not inuse */
      mchunkptr prv = prev_chunk (p);
      assert (next_chunk (prv) == p);
      do_check_free_chunk (av, prv);
    }

  if (next == av->top)
    {
      assert (prev_inuse (next));
      assert (chunksize (next) >= MINSIZE);
    }
  else if (!inuse (next))
    do_check_free_chunk (av, next);
}
```

#### `tcache`

```c
#if USE_TCACHE
  {
    size_t tc_idx = csize2tidx (size);

    if (tcache
	&& tc_idx < mp_.tcache_bins
	&& tcache->counts[tc_idx] < mp_.tcache_count)
      {
	tcache_put (p, tc_idx);
	return;
      }
  }
#endif
```

如果`p`的大小在`tcache bin`范围内，并且`tcache bin`没有满，就会直接被扔进`tcache bin`。

#### `Fastbin && 合并`

1. 检查下一个`chunk`的大小是否在合适范围内`(2 * SIZE_SZ , av->system_mem)`
2. 将`p`放到`fastbin`里，这里会检查Double Free，检查fastbin里的第一个`chunk`是否是`p`。
3. 检查`p`是否是`top_chunk`，如果是，就会`crash`。
4. 且`nextchunk`的位置不能超出`top`的地址范围。检查`nextchunk`的`pre_inuse`是否为`1`。检查`nextchunk`的大小是否在合理范围内。
5. 

首先检查是否在`Fastbin`范围内。

```c
if ((unsigned long)(size) <= (unsigned long)(get_max_fast ())
```

迎接第一个检查，检查下一个`chunk`的大小是否在合适范围内`(2 * SIZE_SZ , av->system_mem)`

```c
  if (__builtin_expect (chunksize_nomask (chunk_at_offset (p, size))
      <= 2 * SIZE_SZ, 0)
|| __builtin_expect (chunksize (chunk_at_offset (p, size))
         >= av->system_mem, 0))
    {
bool fail = true;
/* We might not have a lock at this point and concurrent modifications
   of system_mem might result in a false positive.  Redo the test after
   getting the lock.  */
if (!have_lock)
  {
    __libc_lock_lock (av->mutex);
    fail = (chunksize_nomask (chunk_at_offset (p, size)) <= 2 * SIZE_SZ
      || chunksize (chunk_at_offset (p, size)) >= av->system_mem);
    __libc_lock_unlock (av->mutex);
  }

if (fail)
  malloc_printerr ("free(): invalid next size (fast)");
    }

/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
```

然后这里好像影响不大，不关注。

```c
free_perturb (chunk2mem(p), size - 2 * SIZE_SZ); 

static void
free_perturb (char *p, size_t n)
{
  if (__glibc_unlikely (perturb_byte))
    memset (p, perturb_byte, n);
}
```

找到相应的fastbin的索引`idx`。

```c
unsigned int idx = fastbin_index(size);
fb = &fastbin (av, idx);
```

然后将`p`放到`fastbin`里，这里会检查Double Free，检查fastbin里的第一个`chunk`是否是`p`，

```c
  /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
  mchunkptr old = *fb, old2;

  if (SINGLE_THREAD_P)
    {
/* Check that the top of the bin is not the record we are going to
   add (i.e., double free).  */
if (__builtin_expect (old == p, 0))
  malloc_printerr ("double free or corruption (fasttop)");
p->fd = old;
*fb = p;
    }
  else
    do
{
  /* Check that the top of the bin is not the record we are going to
     add (i.e., double free).  */
  if (__builtin_expect (old == p, 0))
    malloc_printerr ("double free or corruption (fasttop)");
  p->fd = old2 = old;
}
    while ((old = catomic_compare_and_exchange_val_rel (fb, p, old2))
     != old2);

  /* Check that size of fastbin chunk at the top is the same as
     size of the chunk that we are adding.  We can dereference OLD
     only if we have the lock, otherwise it might have already been
     allocated again.  */
  if (have_lock && old != NULL
&& __builtin_expect (fastbin_index (chunksize (old)) != idx, 0))
    malloc_printerr ("invalid fastbin entry (free)");
}
```

**接下来会尝试合并,这部分是作准备，**

会检查`p`是否是`top_chunk`，如果是，就会`crash`，

并且`nextchunk`的位置不能超出`top`的地址范围。检查`nextchunk`的`pre_inuse`是否为`1`。检查`nextchunk`的大小是否在合理范围内。

```c
else if (!chunk_is_mmapped(p)) {

/* If we're single-threaded, don't lock the arena.  */
if (SINGLE_THREAD_P)
  have_lock = true;

if (!have_lock)
  __libc_lock_lock (av->mutex);

nextchunk = chunk_at_offset(p, size);

/* Lightweight tests: check whether the block is already the
   top block.  */
if (__glibc_unlikely (p == av->top))
  malloc_printerr ("double free or corruption (top)");
/* Or whether the next chunk is beyond the boundaries of the arena.  */
if (__builtin_expect (contiguous (av)
    && (char *) nextchunk
    >= ((char *) av->top + chunksize(av->top)), 0))
malloc_printerr ("double free or corruption (out)");
/* Or whether the block is actually not marked used.  */
if (__glibc_unlikely (!prev_inuse(nextchunk)))
  malloc_printerr ("double free or corruption (!prev)");

nextsize = chunksize(nextchunk);
if (__builtin_expect (chunksize_nomask (nextchunk) <= 2 * SIZE_SZ, 0)
|| __builtin_expect (nextsize >= av->system_mem, 0))
  malloc_printerr ("free(): invalid next size (normal)");

free_perturb (chunk2mem(p), size - 2 * SIZE_SZ);
```

接下来开始尝试合并：

```c
/* consolidate backward */
if (!prev_inuse(p)) {
  prevsize = prev_size (p);
  size += prevsize;
  p = chunk_at_offset(p, -((long) prevsize));
  unlink(av, p, bck, fwd);
}
```

如果`pre_chunk`没有在使用，就会与其合并。

```c
/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)					      \
  (((mchunkptr) (((char *) (p)) + (s)))->mchunk_size & PREV_INUSE)

if (nextchunk != av->top) {
    /* get and clear inuse bit */
    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);

    /* consolidate forward */
    if (!nextinuse) {
unlink(av, nextchunk, bck, fwd);
size += nextsize;
    } else
clear_inuse_bit_at_offset(nextchunk, 0);
```

如果`nextchunk`不是`top`，检查`nextchunk`的`nextchunk`的`inuse`位，如果证实`nextchunk`不在使用，就会尝试合并。

```c
    /*
Place the chunk in unsorted chunk list. Chunks are
not placed into regular bins until after they have
been given one chance to be used in malloc.
    */

    bck = unsorted_chunks(av);
    fwd = bck->fd;
    if (__glibc_unlikely (fwd->bk != bck))
malloc_printerr ("free(): corrupted unsorted chunks");
    p->fd = fwd;
    p->bk = bck;
    if (!in_smallbin_range(size))
{
  p->fd_nextsize = NULL;
  p->bk_nextsize = NULL;
}
    bck->fd = p;
    fwd->bk = p;

    set_head(p, size | PREV_INUSE);
    set_foot(p, size);

    check_free_chunk(av, p); //有一个检查，这个不需要咱们操作。
  }
```

然后把这个合并完之后就会把这个新`chunk`放到`Unsorted bin`中，有一个`Unsorted bin`完整性的常规检查。

```c
else {
  size += nextsize;
  set_head(p, size | PREV_INUSE);
  av->top = p;
  check_chunk(av, p);
}
```

如果是`top`，就会和`top`合并。

🤔️如果`size`过大，就会合并`Fastbin`。

```c
#define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)

  if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
    if (atomic_load_relaxed (&av->have_fastchunks))
malloc_consolidate(av);
```

#### 接下来一段🐦

```c

      if (av == &main_arena) {
#ifndef MORECORE_CANNOT_TRIM
	if ((unsigned long)(chunksize(av->top)) >=
	    (unsigned long)(mp_.trim_threshold))
	  systrim(mp_.top_pad, av);
#endif
      } else {
	/* Always try heap_trim(), even if the top chunk is not
	   large, because the corresponding heap might go away.  */
	heap_info *heap = heap_for_ptr(top(av));

	assert(heap->ar_ptr == av);
	heap_trim(heap, mp_.top_pad);
      }
    }

    if (!have_lock)
      __libc_lock_unlock (av->mutex);
  }
  /*
    If the chunk was allocated via mmap, release via munmap().
  */

  else {
    munmap_chunk (p);
  }
}
```

